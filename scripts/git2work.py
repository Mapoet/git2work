#! /usr/bin/env python3

import os
import sys
import subprocess
import argparse
import re
import json
import requests
from collections import defaultdict
from typing import Dict, List, Optional, Tuple
from git import Repo
from datetime import datetime, timedelta, timezone
# API Keys - ä»…ä»ç¯å¢ƒå˜é‡è¯»å–ï¼Œä¸æä¾›é»˜è®¤å€¼ä»¥ç¡®ä¿å®‰å…¨
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
GITHUB_TOKEN = os.getenv("GITHUB_TOKEN")
GITEE_TOKEN = os.getenv("GITEE_TOKEN")
try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    print("Warning: openai package not installed. Please run: pip install openai")
try:
    from github import Github
    try:
        from github import Auth
        GITHUB_AUTH_AVAILABLE = True
    except ImportError:
        GITHUB_AUTH_AVAILABLE = False  # æ—§ç‰ˆæœ¬ PyGithub
    GITHUB_AVAILABLE = True
except ImportError:
    GITHUB_AVAILABLE = False
    GITHUB_AUTH_AVAILABLE = False
    print("Warning: PyGithub package not installed. Please run: pip install PyGithub")


# é»˜è®¤ç³»ç»Ÿæç¤ºè¯
default_system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ€æœ¯æ–‡æ¡£æ’°å†™åŠ©æ‰‹ã€‚æ ¹æ®æä¾›çš„ git commit è®°å½•ï¼Œç”Ÿæˆä¸€ä»½ç»“æ„åŒ–çš„ä¸­æ–‡å·¥ä½œæ€»ç»“ã€‚

è¦æ±‚ï¼š
1. ä½¿ç”¨ Markdown æ ¼å¼
2. æ€»ç»“ä¸»è¦åŒ…æ‹¬ï¼š
   - ä»Šæ—¥å·¥ä½œæ¦‚è¿°ï¼ˆ3-5å¥ï¼‰
   - ä¸»è¦å®Œæˆå†…å®¹ï¼ˆæŒ‰æ¨¡å—åˆ†ç±»ï¼‰
   - ç»Ÿè®¡æ•°æ®ï¼ˆæäº¤æ•°ã€ä»£ç å˜æ›´ã€æ¶‰åŠæ–‡ä»¶ç­‰ï¼‰
   - æŠ€æœ¯äº®ç‚¹æˆ–é‡è¦æ”¹è¿›
3. è¯­è¨€ç®€æ´ä¸“ä¸šï¼Œé¿å…è¿‡äºå†—é•¿
4. é‡ç‚¹å…³æ³¨ä»£ç æ”¹è¿›ã€åŠŸèƒ½å¢å¼ºã€é—®é¢˜ä¿®å¤ç­‰æŠ€æœ¯æ€§å†…å®¹

ã€å·¥ä½œä¼šè¯æ—¶é—´å›¾è¦æ±‚ã€‘
å¦‚æœæä¾›çš„ commit è®°å½•ä¸­åŒ…å«å·¥ä½œä¼šè¯ç»Ÿè®¡ä¿¡æ¯ï¼ˆå¦‚"å·¥ä½œä¼šè¯: X ä¸ªï¼Œæ€»æ—¶é•¿çº¦ Y åˆ†é’Ÿ"ï¼‰ï¼Œè¯·åŸºäºè¿™äº›ä¼šè¯ä¿¡æ¯ç»˜åˆ¶ä¸€ä¸ªç®€æ´çš„å·¥ä½œå†…å®¹æ—¶é—´åˆ†å¸ƒå›¾ã€‚å›¾åº”åŒ…å«ï¼š
1. å„å·¥ä½œä¼šè¯çš„èµ·æ­¢æ—¶é—´èŒƒå›´
2. æ¯ä¸ªä¼šè¯æ¶‰åŠçš„æäº¤æ•°é‡æˆ–ä¸»è¦åŠŸèƒ½æ¨¡å—
3. ä¼šè¯ä¹‹é—´çš„æ—¶é—´é—´éš”ï¼ˆä¾¿äºè¯†åˆ«å·¥ä½œèŠ‚å¥ï¼‰
4. å¦‚æœ‰è·¨é¡¹ç›®æäº¤ï¼Œæ ‡æ³¨é¡¹ç›®åˆ‡æ¢çš„æ—¶é—´ç‚¹
5. **ç‰¹åˆ«æ³¨æ„å¹¶è¡Œå·¥ä½œæ—¶é—´**ï¼šå¦‚æœå­˜åœ¨"è·¨é¡¹ç›®å¹¶è¡Œå·¥ä½œæ—¶é—´æ®µ"æ ‡è¯†ï¼Œè¯·åœ¨æ—¶é—´å›¾ä¸­æ¸…æ™°æ ‡æ³¨åŒæ—¶åœ¨ä¸åŒé¡¹ç›®ä¸Šå·¥ä½œçš„æ—¶æ®µï¼Œè¿™æœ‰åŠ©äºå‡†ç¡®è¯„ä¼°å®é™…æŠ•å…¥æ—¶é—´ï¼ˆå¹¶è¡Œå·¥ä½œä¸åº”ç®€å•ç´¯åŠ ï¼‰

æ—¶é—´å›¾å¯ä½¿ç”¨ Markdown è¡¨æ ¼åŠ Mermaid 10åˆ†é’Ÿçº§ç”˜ç‰¹å›¾å½¢å¼å‘ˆç°ã€‚

è¯·æ ¹æ®æä¾›çš„ commit ä¿¡æ¯ç”Ÿæˆå·¥ä½œæ€»ç»“ã€‚"""

PEI="""\n\næœ€åè®¡ç®—ä¸€ä¸‹æ•ˆç‡æŒ‡æ•°ï¼ˆPEIï¼‰ï¼š
        è®¾ï¼š
* $N_c$ = å½“æ—¥æäº¤æ¬¡æ•°
* $L_{add}$ = æ–°å¢ä»£ç è¡Œæ•°
* $L_{del}$ = åˆ é™¤ä»£ç è¡Œæ•°
* $T$ = å®é™…æŠ•å…¥æ—¶é—´ï¼ˆå°æ—¶ï¼Œæ’é™¤å¹¶è¡Œé‡å ï¼‰
* $P_{mod}$ = ä¿®æ”¹æ–‡ä»¶æ•°
* $C_{eff}$ = ç¼–è¯‘é€šè¿‡ç‡ï¼ˆæˆ–æµ‹è¯•é€šè¿‡ç‡ï¼Œ0~1ï¼‰
* $C_{cmp}$ = ä»£ç å¤æ‚åº¦ç³»æ•°ï¼ˆ0.5~1.5ï¼Œå¯ä¾æ®ä»»åŠ¡ç±»å‹è°ƒæ•´ï¼‰
---
å…¬å¼ï¼š
$$
\\text{PEI} = \\frac{(0.4 N_c + 0.3 \\log_{10}(L_{add}+L_{del}) + 0.2 \\log_{10}(P_{mod}+1)) \\times C_{eff} \\times C_{cmp}}{T/8}
$$
> è¯´æ˜ï¼š
>
> * å¯¹æ•°é¡¹ä½¿å¾—ä»£ç é‡å’Œæ–‡ä»¶æ•°å¸¦æ¥é€’å‡æ•ˆç›Šï¼Œé˜²æ­¢è¡Œæ•°å †ç§¯é€ æˆè™šé«˜ã€‚
> * $T/8$ ç”¨äºæ—¶é—´å½’ä¸€åŒ–ï¼ˆä»¥ 8 å°æ—¶ä¸ºæ ‡å‡†å·¥ä½œæ—¥ï¼‰ã€‚
> * ç³»æ•°å¯è°ƒï¼š`0.4/0.3/0.2` æƒé‡é€‚åˆä¸­å‹é¡¹ç›®ï¼ˆå¦‚C++å·¥ç¨‹ï¼‰ã€‚
å‚è€ƒè§£é‡Šè¡¨

| PEI å€¼ | æ•ˆç‡ç­‰çº§  | ç‰¹å¾æè¿°           |
| ----- | ----- | -------------- |
| 0â€“3   | ğŸ’¤ ä½æ•ˆ | é¢‘ç¹ä¸Šä¸‹æ–‡åˆ‡æ¢ã€éæ ¸å¿ƒä»»åŠ¡  |
| 4â€“6   | âš™ï¸ æ­£å¸¸ | æŒç»­æ¨è¿›ã€ç¨³å®šäº§å‡º      |
| 7â€“9   | ğŸš€ é«˜æ•ˆ | æ¨¡å—é‡æ„ã€ç³»ç»Ÿä¼˜åŒ–æˆ–å…³é”®ä¿®å¤ |
| â‰¥10   | ğŸ§  å“è¶Š | è‡ªåŠ¨åŒ–ã€ç”Ÿæˆå¼ä»»åŠ¡ã€é›†ä¸­æ”»åš |
        """

def get_github_events(repo_full_name: str, token: str, since_dt: datetime, until_dt: datetime) -> List[Dict]:
    """
    ä» GitHub è·å–æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„ commits å’Œ PRsã€‚
    
    Args:
        repo_full_name: ä»“åº“å…¨åï¼Œæ ¼å¼ä¸º "OWNER/REPO"
        token: GitHub Personal Access Token
        since_dt: èµ·å§‹æ—¶é—´ï¼ˆdatetimeï¼Œå»ºè®®å¸¦æ—¶åŒºï¼‰
        until_dt: ç»“æŸæ—¶é—´ï¼ˆdatetimeï¼Œå»ºè®®å¸¦æ—¶åŒºï¼‰
    
    Returns:
        äº‹ä»¶åˆ—è¡¨ï¼Œæ ¼å¼ä¸æœ¬åœ° commit å…¼å®¹ï¼š
        [{
            "sha": commit_sha æˆ– "PR#123",
            "author_name": author_name,
            "author_email": "" (è¿œç¨‹ä»“åº“é€šå¸¸æ²¡æœ‰email),
            "date": date_str (ISOæ ¼å¼å­—ç¬¦ä¸²),
            "date_epoch": epoch_seconds,
            "message": commit_message æˆ– pr_title,
            "type": "commit" æˆ– "pr"
        }, ...]
    """
    if not GITHUB_AVAILABLE:
        raise ImportError("PyGithub æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install PyGithub")
    
    events: List[Dict] = []
    # ä½¿ç”¨æ–°çš„è®¤è¯æ–¹å¼ï¼ˆé¿å… deprecation warningï¼‰
    if GITHUB_AUTH_AVAILABLE:
        auth = Auth.Token(token)
        g = Github(auth=auth)
    else:
        # æ—§ç‰ˆæœ¬ PyGithubï¼Œä½¿ç”¨æ—§çš„æ–¹å¼
        g = Github(token)
    
    try:
        repo = g.get_repo(repo_full_name)
    except Exception as e:
        error_msg = str(e)
        if "403" in error_msg or "Forbidden" in error_msg:
            raise Exception(
                f"æ— æ³•è®¿é—®ä»“åº“ {repo_full_name}ã€‚å¯èƒ½åŸå› ï¼š\n"
                f"1. ä»“åº“æ˜¯ç§æœ‰çš„ï¼Œä¸” token æ²¡æœ‰è®¿é—®æƒé™\n"
                f"2. token æƒé™ä¸è¶³ï¼ˆéœ€è¦ 'repo' æƒé™æ¥è®¿é—®ç§æœ‰ä»“åº“ï¼‰\n"
                f"3. ä»“åº“ä¸å­˜åœ¨æˆ– token æ— æ•ˆ\n"
                f"è¯·æ£€æŸ¥ token æƒé™è®¾ç½®ï¼šhttps://github.com/settings/tokens"
            )
        raise
    
    # ç¡®ä¿æ—¶åŒºä¸º UTC
    since_utc = since_dt.replace(tzinfo=timezone.utc) if since_dt.tzinfo is None else since_dt.astimezone(timezone.utc)
    until_utc = until_dt.replace(tzinfo=timezone.utc) if until_dt.tzinfo is None else until_dt.astimezone(timezone.utc)
    
    # 1) è·å– Commits
    try:
        commits_iter = repo.get_commits(since=since_utc, until=until_utc)
        for c in commits_iter:
            commit_date = c.commit.author.date
            if commit_date.tzinfo is None:
                commit_date = commit_date.replace(tzinfo=timezone.utc)
            
            # åªåŒ…å«æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„æäº¤
            if since_utc <= commit_date <= until_utc:
                message = c.commit.message.splitlines()[0] if c.commit.message else ""
                # å°è¯•è·å–ä½œè€…åç§°ï¼šå…ˆå°è¯• nameï¼Œå†å°è¯• committer çš„ login
                author_name = getattr(c.commit.author, "name", None)
                if not author_name:
                    try:
                        author_name = c.commit.committer.login if hasattr(c.commit.committer, "login") else "Unknown"
                    except:
                        author_name = "Unknown"
                events.append({
                    "sha": c.sha,
                    "author_name": author_name or "Unknown",
                    "author_email": "",  # GitHub API é€šå¸¸ä¸æä¾›é‚®ç®±
                    "date": commit_date.isoformat(),
                    "date_epoch": int(commit_date.timestamp()),
                    "message": message,
                    "type": "commit"
                })
    except Exception as e:
        error_msg = str(e)
        if "403" in error_msg or "Forbidden" in error_msg:
            print(f"Warning: è·å– GitHub ä»“åº“ {repo_full_name} çš„ commits å¤±è´¥: æƒé™ä¸è¶³")
            print(f"æç¤º: å¦‚æœæ˜¯ç§æœ‰ä»“åº“ï¼Œè¯·ç¡®ä¿ token å…·æœ‰ 'repo' æƒé™")
            print(f"æç¤º: å¦‚æœæ˜¯å…¬å¼€ä»“åº“ï¼Œå¯èƒ½æ˜¯ token æƒé™é—®é¢˜ï¼Œæˆ–ä»“åº“ä¸å­˜åœ¨")
        elif "404" in error_msg or "Not Found" in error_msg:
            print(f"Warning: è·å– GitHub ä»“åº“ {repo_full_name} çš„ commits å¤±è´¥: ä»“åº“æœªæ‰¾åˆ°")
        else:
            print(f"Warning: è·å– GitHub commits å¤±è´¥: {e}")
    
    # 2) è·å– PRsï¼ˆé€šè¿‡æœç´¢æ¥å£ï¼ŒæŒ‰ updated æ—¶é—´èŒƒå›´ï¼‰
    try:
        query = f"repo:{repo_full_name} is:pr updated:{since_utc.date()}..{until_utc.date()}"
        for pr in g.search_issues(query=query):
            pr_updated = pr.updated_at
            if pr_updated.tzinfo is None:
                pr_updated = pr_updated.replace(tzinfo=timezone.utc)
            
            # æ£€æŸ¥æ˜¯å¦åœ¨æ—¶é—´èŒƒå›´å†…
            if since_utc <= pr_updated <= until_utc:
                events.append({
                    "sha": f"PR#{pr.number}",
                    "author_name": pr.user.login if pr.user else "Unknown",
                    "author_email": "",
                    "date": pr_updated.isoformat(),
                    "date_epoch": int(pr_updated.timestamp()),
                    "message": pr.title,
                    "type": "pr"
                })
    except Exception as e:
        error_msg = str(e)
        if "403" in error_msg or "Forbidden" in error_msg:
            print(f"Warning: è·å– GitHub ä»“åº“ {repo_full_name} çš„ PRs å¤±è´¥: æƒé™ä¸è¶³")
            print(f"æç¤º: è¯·ç¡®ä¿ token å…·æœ‰è®¿é—®ä»“åº“çš„æƒé™")
        elif "404" in error_msg or "Not Found" in error_msg:
            print(f"Warning: è·å– GitHub ä»“åº“ {repo_full_name} çš„ PRs å¤±è´¥: ä»“åº“æœªæ‰¾åˆ°")
        else:
            print(f"Warning: è·å– GitHub PRs å¤±è´¥: {e}")
    
    # æŒ‰æ—¶é—´æ’åº
    events.sort(key=lambda e: e["date_epoch"])
    return events

def get_gitee_events(repo_full_name: str, token: str, since_dt: datetime, until_dt: datetime) -> List[Dict]:
    """
    ä» Gitee è·å–æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„ commits å’Œ PRsï¼ˆMRsï¼‰ã€‚
    
    Args:
        repo_full_name: ä»“åº“å…¨åï¼Œæ ¼å¼ä¸º "OWNER/REPO"
        token: Gitee Personal Access Token
        since_dt: èµ·å§‹æ—¶é—´ï¼ˆdatetimeï¼Œå»ºè®®å¸¦æ—¶åŒºï¼‰
        until_dt: ç»“æŸæ—¶é—´ï¼ˆdatetimeï¼Œå»ºè®®å¸¦æ—¶åŒºï¼‰
    
    Returns:
        äº‹ä»¶åˆ—è¡¨ï¼Œæ ¼å¼ä¸æœ¬åœ° commit å…¼å®¹
    """
    events: List[Dict] = []
    
    # ç¡®ä¿æ—¶åŒºä¸º UTC
    since_utc = since_dt.replace(tzinfo=timezone.utc) if since_dt.tzinfo is None else since_dt.astimezone(timezone.utc)
    until_utc = until_dt.replace(tzinfo=timezone.utc) if until_dt.tzinfo is None else until_dt.astimezone(timezone.utc)
    
    owner, repo_name = repo_full_name.split("/", 1)
    base_url = "https://gitee.com/api/v5"
    headers = {"Authorization": f"token {token}"} if token else {}
    
    # 1) è·å– Commits
    try:
        commits_url = f"{base_url}/repos/{owner}/{repo_name}/commits"
        params = {
            "since": since_utc.isoformat(),
            "until": until_utc.isoformat(),
            "per_page": 100,
            "page": 1
        }
        
        page = 1
        while True:
            params["page"] = page
            resp = requests.get(commits_url, headers=headers, params=params, timeout=30)
            resp.raise_for_status()
            commits_data = resp.json()
            
            if not commits_data:
                break
            
            for c in commits_data:
                commit_date_str = c.get("commit", {}).get("author", {}).get("date", "")
                if commit_date_str:
                    try:
                        commit_date = datetime.fromisoformat(commit_date_str.replace("Z", "+00:00"))
                        if commit_date.tzinfo is None:
                            commit_date = commit_date.replace(tzinfo=timezone.utc)
                        
                        # åªåŒ…å«æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„æäº¤
                        if since_utc <= commit_date <= until_utc:
                            message = c.get("commit", {}).get("message", "").splitlines()[0] if c.get("commit", {}).get("message") else ""
                            author_info = c.get("commit", {}).get("author", {})
                            author_name = author_info.get("name", "Unknown")
                            
                            events.append({
                                "sha": c.get("sha", "")[:40],
                                "author_name": author_name,
                                "author_email": author_info.get("email", ""),
                                "date": commit_date.isoformat(),
                                "date_epoch": int(commit_date.timestamp()),
                                "message": message,
                                "type": "commit"
                            })
                    except Exception as e:
                        print(f"Warning: è§£æ Gitee commit æ—¶é—´å¤±è´¥: {e}")
                        continue
            
            if len(commits_data) < 100:
                break
            page += 1
    except Exception as e:
        print(f"Warning: è·å– Gitee commits å¤±è´¥: {e}")
    
    # 2) è·å– Pull Requests (MRs)
    try:
        mrs_url = f"{base_url}/repos/{owner}/{repo_name}/pulls"
        params = {
            "state": "all",
            "sort": "updated",
            "direction": "desc",
            "per_page": 100,
            "page": 1
        }
        
        page = 1
        while True:
            params["page"] = page
            resp = requests.get(mrs_url, headers=headers, params=params, timeout=30)
            resp.raise_for_status()
            mrs_data = resp.json()
            
            if not mrs_data:
                break
            
            for mr in mrs_data:
                updated_str = mr.get("updated_at", "")
                if updated_str:
                    try:
                        updated_date = datetime.fromisoformat(updated_str.replace("Z", "+00:00"))
                        if updated_date.tzinfo is None:
                            updated_date = updated_date.replace(tzinfo=timezone.utc)
                        
                        # åªåŒ…å«æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„ PR
                        if since_utc <= updated_date <= until_utc:
                            events.append({
                                "sha": f"PR#{mr.get('number', '')}",
                                "author_name": mr.get("user", {}).get("login", "Unknown"),
                                "author_email": "",
                                "date": updated_date.isoformat(),
                                "date_epoch": int(updated_date.timestamp()),
                                "message": mr.get("title", ""),
                                "type": "pr"
                            })
                    except Exception as e:
                        print(f"Warning: è§£æ Gitee PR æ—¶é—´å¤±è´¥: {e}")
                        continue
            
            # å¦‚æœæœ€æ—©çš„ PR æ›´æ–°æ—¶é—´æ—©äºæŸ¥è¯¢èŒƒå›´ï¼Œå¯ä»¥æå‰é€€å‡º
            try:
                earliest_updated = min(
                    (datetime.fromisoformat(mr.get("updated_at", "").replace("Z", "+00:00")) 
                     for mr in mrs_data if mr.get("updated_at")),
                    default=None
                )
                if earliest_updated and earliest_updated < since_utc:
                    break
            except Exception:
                pass  # å¦‚æœè§£æå¤±è´¥ï¼Œç»§ç»­ä¸‹ä¸€é¡µ
            
            if len(mrs_data) < 100:
                break
            page += 1
    except Exception as e:
        print(f"Warning: è·å– Gitee PRs å¤±è´¥: {e}")
    
    # æŒ‰æ—¶é—´æ’åº
    events.sort(key=lambda e: e["date_epoch"])
    return events

def parse_git_log(raw):
    # æˆ‘ä»¬ä½¿ç”¨ git log è¾“å‡ºä»¥ \x1eï¼ˆrecord sepï¼‰åˆ†å‰² commitï¼Œä»¥ \x1f å­—æ®µåˆ†å‰²
    commits = []
    if not raw:
        return commits
    for entry in raw.strip("\x1e").split("\x1e"):
        parts = entry.split("\x1f")
        if len(parts) < 5:
            continue
        # æ”¯æŒä¸¤ç§æ ¼å¼ï¼š5å­—æ®µ(æ—§) æˆ– 6å­—æ®µ(å« %at epoch)
        if len(parts) >= 6:
            sha, author_name, author_email, date_str, epoch_str, message = [p.strip() for p in parts[:6]]
            date_epoch = int(epoch_str) if epoch_str.isdigit() else None
        else:
            sha, author_name, author_email, date_str, message = [p.strip() for p in parts[:5]]
            date_epoch = None
        # date_str ç¤ºä¾‹: 2025-10-20 12:34:56 +0800 ï¼ˆå–å†³äº --date=isoï¼‰
        commits.append({
            "sha": sha,
            "author_name": author_name,
            "author_email": author_email,
            "date": date_str,
            "date_epoch": date_epoch,
            "message": message,
        })
    return commits

def get_commits_between(repo_path, since_dt, until_dt, max_count=None):
    """
    since_dt / until_dt: python datetimeï¼ˆæœ€å¥½å¸¦æ—¶åŒºæˆ–è€…æ˜¯æœ¬åœ°æ—¶é—´ï¼‰
    è¿”å› commit å¯¹è±¡åˆ—è¡¨ï¼ˆå¯ä»¥è½¬æ¢ä¸º dictï¼‰
    """
    repo = Repo(repo_path)
    # gitpython æ²¡æœ‰ç›´æ¥å‚æ•°ç”¨æ¥ç­›é€‰æ—¥æœŸï¼Œæ‰€ä»¥ä½¿ç”¨ git directly via repo.git.log æ›´æ–¹ä¾¿ï¼š
    since = since_dt.isoformat(sep=' ')
    until = until_dt.isoformat(sep=' ')
    # å¢åŠ  %atï¼ˆauthor epoch ç§’ï¼‰ä¾¿äºç¨³å®šæ—¶é—´ç»Ÿè®¡
    raw = repo.git.log(
        f'--since={since}',
        f'--until={until}',
        '--pretty=format:%H%x1f%an%x1f%ae%x1f%ad%x1f%at%x1f%s%x1e',
        date='iso'
    )
    # å¤ç”¨ä¸Šé¢ parse å‡½æ•°
    return parse_git_log(raw)

def get_commit_numstat(repo_path: str, sha: str) -> Tuple[List[str], int, int]:
    """
    è¿”å› (files, insertions, deletions)
    é€šè¿‡ `git show --numstat` è§£ææ¯ä¸ª commit ä¿®æ”¹çš„æ–‡ä»¶ä¸å¢åˆ è¡Œæ•°ã€‚
    """
    repo = Repo(repo_path)
    # --pretty=tformat: åªè¾“å‡ºæ–‡ä»¶å˜æ›´ï¼ˆé¿å…é‡å¤å…ƒä¿¡æ¯ï¼‰
    output = repo.git.show(sha, '--numstat', '--pretty=tformat:')
    files: List[str] = []
    insertions_total = 0
    deletions_total = 0
    for line in output.splitlines():
        parts = line.split('\t')
        if len(parts) == 3:
            add_str, del_str, path = parts
            # äºŒè¿›åˆ¶æ–‡ä»¶ä¼šæ˜¾ç¤º '-'ï¼Œæ­¤æ—¶è®¡ä¸º 0
            try:
                add = int(add_str) if add_str.isdigit() else 0
            except ValueError:
                add = 0
            try:
                dele = int(del_str) if del_str.isdigit() else 0
            except ValueError:
                dele = 0
            insertions_total += add
            deletions_total += dele
            files.append(path)
    return files, insertions_total, deletions_total

def get_commit_body(repo_path: str, sha: str) -> str:
    """
    è·å–å®Œæ•´ commit messageï¼ˆå«ä¸»é¢˜ä¸æ­£æ–‡ï¼‰ã€‚
    """
    repo = Repo(repo_path)
    body = repo.git.show(sha, '-s', '--format=%B')
    return body.strip('\n')

def get_pull_operations(repo_path: str, since_dt: datetime, until_dt: datetime) -> List[datetime]:
    """
    è·å–æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„ git pull/fetch æ“ä½œæ—¶é—´ã€‚
    
    ä½¿ç”¨ git reflog è·å–æ“ä½œå†å²ï¼ŒæŸ¥æ‰¾ pullã€fetchã€merge ç­‰æ“ä½œã€‚
    reflog æ ¼å¼: <hash> HEAD@{<timestamp>}: <operation>: <message>
    
    Args:
        repo_path: Git ä»“åº“è·¯å¾„
        since_dt: èµ·å§‹æ—¶é—´
        until_dt: ç»“æŸæ—¶é—´
    
    Returns:
        pull æ“ä½œæ—¶é—´åˆ—è¡¨ï¼ˆæŒ‰æ—¶é—´æ’åºï¼‰
    """
    try:
        repo = Repo(repo_path)
        
        since_iso = since_dt.isoformat(sep=' ')
        until_iso = until_dt.isoformat(sep=' ')
        
        # è·å–æ‰€æœ‰ reflog è®°å½•ï¼ˆä½¿ç”¨ HEAD@{date} æ ¼å¼ï¼‰
        try:
            # å°è¯•è·å– reflogï¼ˆæŸäº›ä»“åº“å¯èƒ½æ²¡æœ‰ reflogï¼‰
            # reflog è¾“å‡ºæ ¼å¼: <hash> HEAD@{2025-11-03 01:26:20 +0800}: pull: Fast-forward
            reflog_output = repo.git.reflog(
                '--date=iso',
                f'--since={since_iso}',
                f'--until={until_iso}'
            )
        except Exception:
            # å¦‚æœæ²¡æœ‰ reflog æˆ–è·å–å¤±è´¥ï¼Œè¿”å›ç©ºåˆ—è¡¨
            return []
        
        if not reflog_output:
            return []
        
        pull_times: List[datetime] = []
        
        # è§£æ reflog è¾“å‡º
        # æ ¼å¼: <hash> HEAD@{<timestamp>}: <operation>: <message>
        # ç¤ºä¾‹: 9bef194 HEAD@{2025-11-03 01:26:20 +0800}: pull: Fast-forward
        for line in reflog_output.splitlines():
            if not line.strip():
                continue
            
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–æ—¶é—´æˆ³å’Œæ“ä½œä¿¡æ¯
            # åŒ¹é…æ ¼å¼: HEAD@{YYYY-MM-DD HH:MM:SS +TZ}: <operation>:
            match = re.search(r'HEAD@\{([^\}]+)\}:\s*([^:]+):', line)
            if match:
                date_str = match.group(1).strip()
                operation = match.group(2).strip().lower()
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯ pull/fetch ç›¸å…³æ“ä½œ
                # pull é€šå¸¸åŒ…å« "pull", "fetch", "merge" ç­‰å…³é”®è¯
                is_pull_related = any(keyword in operation for keyword in [
                    'pull', 'fetch', 'merge', 'update', 'rebase'
                ])
                
                # æ’é™¤ä¸€äº›ä¸ç›¸å…³çš„æ“ä½œï¼ˆå¦‚ checkout, commit, reset ç­‰ï¼‰
                excluded_keywords = ['checkout', 'commit', 'reset', 'branch', 'switch']
                if any(keyword in operation for keyword in excluded_keywords):
                    is_pull_related = False
                
                if is_pull_related:
                    try:
                        # è§£ææ—¥æœŸå­—ç¬¦ä¸²ï¼ˆISO æ ¼å¼ï¼š2025-11-03 01:26:20 +0800ï¼‰
                        pull_time = datetime.strptime(date_str, "%Y-%m-%d %H:%M:%S %z")
                        # è½¬æ¢ä¸ºæœ¬åœ°æ—¶é—´ï¼ˆå»æ‰æ—¶åŒºä¿¡æ¯ä»¥ä¾¿æ¯”è¾ƒï¼‰
                        pull_time = pull_time.astimezone().replace(tzinfo=None)
                        
                        # ç¡®ä¿åœ¨æ—¶é—´èŒƒå›´å†…
                        since_local = since_dt.replace(tzinfo=None) if since_dt.tzinfo else since_dt
                        until_local = until_dt.replace(tzinfo=None) if until_dt.tzinfo else until_dt
                        
                        if since_local <= pull_time <= until_local:
                            pull_times.append(pull_time)
                    except Exception as e:
                        # è§£æå¤±è´¥ï¼Œè·³è¿‡
                        continue
        
        # å»é‡å¹¶æ’åºï¼ˆä»æ—©åˆ°æ™šï¼‰
        pull_times = sorted(list(set(pull_times)))
        return pull_times
        
    except Exception as e:
        # è·å–å¤±è´¥ï¼Œè¿”å›ç©ºåˆ—è¡¨ï¼ˆä¸é˜»æ–­ä¸»æµç¨‹ï¼‰
        print(f"Warning: è·å–ä»“åº“ {repo_path} çš„ pull è®°å½•å¤±è´¥: {e}")
        return []

def group_commits_by_date(commits: List[Dict]) -> Dict[str, List[Dict]]:
    groups: Dict[str, List[Dict]] = defaultdict(list)
    for c in commits:
        # date å­—ç¬¦ä¸²å½¢å¦‚ "2025-10-20 12:34:56 +0800"
        date_part = c['date'].split(' ')[0]
        groups[date_part].append(c)
    # å¯¹æ¯ç»„æŒ‰æ—¶é—´æ’åºï¼ˆä»æ—©åˆ°æ™šï¼‰
    for k in groups:
        groups[k].sort(key=lambda x: x['date'])
    return dict(sorted(groups.items(), key=lambda x: x[0]))

def commit_time_dt(c: Dict) -> datetime:
    if c.get('date_epoch'):
        try:
            return datetime.fromtimestamp(int(c['date_epoch']))
        except Exception:
            pass
    # Fallback parse from string
    ds = c.get('date', '')
    try:
        return datetime.strptime(ds, "%Y-%m-%d %H:%M:%S %z").astimezone().replace(tzinfo=None)
    except Exception:
        try:
            return datetime.strptime(ds, "%Y-%m-%d %H:%M:%S")
        except Exception:
            return datetime.fromisoformat(ds.replace(' ', 'T').split(' +')[0])

def compute_work_sessions(commits: List[Dict], gap_minutes: int = 60, pull_times: Optional[List[datetime]] = None) -> List[Dict]:
    """
    è®¡ç®—å·¥ä½œä¼šè¯ï¼Œæ”¯æŒä½¿ç”¨ pull æ—¶é—´ä½œä¸ºä¼šè¯å¼€å§‹æ—¶é—´ã€‚
    
    Args:
        commits: commit åˆ—è¡¨
        gap_minutes: ä¼šè¯é—´éš”ï¼ˆåˆ†é’Ÿï¼‰
        pull_times: pull æ“ä½œæ—¶é—´åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
    
    Returns:
        å·¥ä½œä¼šè¯åˆ—è¡¨
    """
    if not commits:
        return []
    items = sorted(commits, key=lambda c: commit_time_dt(c))
    sessions: List[Dict] = []
    gap = timedelta(minutes=gap_minutes)
    
    # å¦‚æœæœ‰ pull è®°å½•ï¼Œç”¨äºè°ƒæ•´ä¼šè¯å¼€å§‹æ—¶é—´
    pull_times_sorted = sorted(pull_times) if pull_times else []
    
    current = {
        'start': commit_time_dt(items[0]),
        'end': commit_time_dt(items[0]),
        'commits': [items[0]],
    }
    
    # ä¸ºç¬¬ä¸€ä¸ªä¼šè¯æŸ¥æ‰¾å¯¹åº”çš„ pull æ—¶é—´
    # å¦‚æœç¬¬ä¸€ä¸ª commit ä¹‹å‰æœ‰ pull æ“ä½œï¼Œä¸”æ—¶é—´é—´éš”åˆç†ï¼ˆä¸è¶…è¿‡ gap_minutesï¼‰ï¼Œä½¿ç”¨ pull æ—¶é—´ä½œä¸ºå¼€å§‹
    first_commit_time = commit_time_dt(items[0])
    if pull_times_sorted:
        # æŸ¥æ‰¾ç¬¬ä¸€ä¸ª commit ä¹‹å‰æœ€è¿‘çš„ pull æ“ä½œ
        # æ‰¾åˆ°ç¬¬ä¸€ä¸ªåœ¨ commit æ—¶é—´ä¹‹å‰çš„ pullï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        for pull_time in reversed(pull_times_sorted):
            if pull_time <= first_commit_time:
                # æ£€æŸ¥æ—¶é—´é—´éš”æ˜¯å¦åˆç†ï¼ˆpull æ—¶é—´åº”è¯¥åœ¨ commit ä¹‹å‰ï¼Œä½†ä¸è¦ç›¸éš”å¤ªä¹…ï¼‰
                time_diff = (first_commit_time - pull_time).total_seconds() / 60
                # å¦‚æœ pull åœ¨ commit ä¹‹å‰ä¸”åœ¨åˆç†èŒƒå›´å†…ï¼ˆæ¯”å¦‚ 2 å°æ—¶å†…ï¼‰ï¼Œä½¿ç”¨ pull æ—¶é—´
                if time_diff > 0 and time_diff <= 120:  # 2 å°æ—¶å†…çš„ pull è§†ä¸ºæœ‰æ•ˆ
                    current['start'] = pull_time
                    break
    
    for c in items[1:]:
        t = commit_time_dt(c)
        if t - commit_time_dt(current['commits'][-1]) <= gap:
            current['end'] = t
            current['commits'].append(c)
        else:
            # ä¼šè¯ç»“æŸï¼Œè®¡ç®—æ—¶é•¿
            current['duration_minutes'] = max(1, int((current['end'] - current['start']).total_seconds() // 60))
            sessions.append(current)
            
            # å¼€å§‹æ–°ä¼šè¯
            current = {'start': t, 'end': t, 'commits': [c]}
            
            # ä¸ºæ–°ä¼šè¯æŸ¥æ‰¾å¯¹åº”çš„ pull æ—¶é—´
            if pull_times_sorted:
                # æŸ¥æ‰¾è¿™ä¸ª commit ä¹‹å‰æœ€è¿‘çš„ pull æ“ä½œ
                for pull_time in reversed(pull_times_sorted):
                    if pull_time <= t:
                        time_diff = (t - pull_time).total_seconds() / 60
                        if time_diff > 0 and time_diff <= 120:  # 2 å°æ—¶å†…çš„ pull è§†ä¸ºæœ‰æ•ˆ
                            current['start'] = pull_time
                            break
    
    # å¤„ç†æœ€åä¸€ä¸ªä¼šè¯
    current['duration_minutes'] = max(1, int((current['end'] - current['start']).total_seconds() // 60))
    sessions.append(current)
    return sessions

def compute_feature_windows(commits: List[Dict]) -> Dict[str, Dict]:
    # Group by leading token of commit message (e.g., feat, fix, docs, build, refactor)
    windows: Dict[str, Dict] = {}
    for c in commits:
        msg = c.get('message', '').strip()
        token = msg.split(':', 1)[0].lower().split(' ')[0]
        key = token if token in ['feat', 'fix', 'docs', 'build', 'refactor', 'chore', 'perf', 'test'] else 'other'
        t = commit_time_dt(c)
        w = windows.get(key)
        if not w:
            windows[key] = {'start': t, 'end': t, 'count': 1}
        else:
            if t < w['start']:
                w['start'] = t
            if t > w['end']:
                w['end'] = t
            w['count'] += 1
    return windows

def detect_parallel_sessions(repo_to_sessions: Dict[str, List[Dict]]) -> List[Dict]:
    """
    æ£€æµ‹è·¨é¡¹ç›®çš„å¹¶è¡Œå·¥ä½œæ—¶æ®µã€‚
    è¿”å›é‡å çš„æ—¶é—´æ®µåŠå…¶æ¶‰åŠçš„é¡¹ç›®åˆ—è¡¨ã€‚
    """
    if len(repo_to_sessions) < 2:
        return []  # å•é¡¹ç›®ä¸éœ€è¦æ£€æµ‹å¹¶è¡Œ
    
    all_periods: List[Dict] = []
    for repo, sessions in repo_to_sessions.items():
        for s in sessions:
            all_periods.append({
                'start': s['start'],
                'end': s['end'],
                'repo': repo,
                'session': s
            })
    
    if not all_periods:
        return []
    
    # åˆå¹¶é‡å æ—¶æ®µç®—æ³•ï¼šæŒ‰æ—¶é—´çº¿æ‰«æï¼Œåˆå¹¶è¿ç»­æˆ–é‡å çš„æ—¶æ®µ
    all_periods.sort(key=lambda x: (x['start'], x['end']))
    merged_overlaps: List[Dict] = []
    
    current_overlaps = []  # å½“å‰æ­£åœ¨é‡å çš„æ—¶æ®µç»„
    
    for period in all_periods:
        if not current_overlaps:
            current_overlaps = [period]
            continue
        
        # æ£€æŸ¥å½“å‰æ—¶æ®µæ˜¯å¦ä¸å·²æœ‰é‡å ç»„æœ‰æ—¶é—´é‡å 
        can_merge = False
        for existing in current_overlaps:
            if not (period['end'] < existing['start'] or period['start'] > existing['end']):
                can_merge = True
                break
        
        if can_merge:
            current_overlaps.append(period)
        else:
            # ç»“æŸå½“å‰é‡å ç»„ï¼Œå¼€å§‹æ–°çš„
            if len(set(p['repo'] for p in current_overlaps)) > 1:
                overlap_start = min(p['start'] for p in current_overlaps)
                overlap_end = max(p['end'] for p in current_overlaps)
                overlap_repos = sorted(set(p['repo'] for p in current_overlaps))
                merged_overlaps.append({
                    'start': overlap_start,
                    'end': overlap_end,
                    'repos': overlap_repos,
                    'duration_minutes': int((overlap_end - overlap_start).total_seconds() // 60)
                })
            current_overlaps = [period]
    
    # å¤„ç†æœ€åä¸€ç»„
    if len(set(p['repo'] for p in current_overlaps)) > 1:
        overlap_start = min(p['start'] for p in current_overlaps)
        overlap_end = max(p['end'] for p in current_overlaps)
        overlap_repos = sorted(set(p['repo'] for p in current_overlaps))
        merged_overlaps.append({
            'start': overlap_start,
            'end': overlap_end,
            'repos': overlap_repos,
            'duration_minutes': int((overlap_end - overlap_start).total_seconds() // 60)
        })
    
    # å†æ¬¡åˆå¹¶å¯èƒ½è¿ç»­æˆ–éƒ¨åˆ†é‡å çš„æ—¶æ®µ
    if not merged_overlaps:
        return []
    
    final_merged: List[Dict] = []
    merged_overlaps.sort(key=lambda x: (x['start'], x['end']))
    
    current = merged_overlaps[0]
    for next_period in merged_overlaps[1:]:
        # å¦‚æœæ—¶é—´æœ‰é‡å æˆ–è¿ç»­ï¼ˆé—´éš”å°äº5åˆ†é’Ÿè§†ä¸ºè¿ç»­ï¼‰ï¼Œä¸”æ¶‰åŠç›¸åŒé¡¹ç›®ï¼Œåˆ™åˆå¹¶
        gap = (next_period['start'] - current['end']).total_seconds() / 60
        if gap <= 5 or not (next_period['end'] < current['start'] or next_period['start'] > current['end']):
            # åˆå¹¶
            current['start'] = min(current['start'], next_period['start'])
            current['end'] = max(current['end'], next_period['end'])
            current['repos'] = sorted(set(current['repos']) | set(next_period['repos']))
            current['duration_minutes'] = int((current['end'] - current['start']).total_seconds() // 60)
        else:
            final_merged.append(current)
            current = next_period
    final_merged.append(current)
    
    return final_merged

def build_commit_context_by_project(repo_to_grouped: Dict[str, Dict[str, List[Dict]]], repo_to_details: Dict[str, Dict[str, Tuple[List[str], int, int, str]]], gap_minutes: int = 60, repo_to_pull_times: Optional[Dict[str, List[datetime]]] = None) -> str:
    lines: List[str] = []
    
    # å…ˆè®¡ç®—æ‰€æœ‰é¡¹ç›®çš„ä¼šè¯ï¼Œç”¨äºæ£€æµ‹å¹¶è¡Œå·¥ä½œ
    repo_to_sessions: Dict[str, List[Dict]] = {}
    for repo_name, grouped in repo_to_grouped.items():
        flat_commits: List[Dict] = []
        for items in grouped.values():
            flat_commits.extend(items)
        # è·å–è¯¥ä»“åº“çš„ pull æ—¶é—´ï¼ˆå¦‚æœæ˜¯æœ¬åœ°ä»“åº“ï¼‰
        pull_times = repo_to_pull_times.get(repo_name, []) if repo_to_pull_times else []
        sessions = compute_work_sessions(flat_commits, gap_minutes, pull_times)
        repo_to_sessions[repo_name] = sessions
    
    # æ£€æµ‹è·¨é¡¹ç›®å¹¶è¡Œå·¥ä½œæ—¶é—´
    parallel_periods = detect_parallel_sessions(repo_to_sessions)
    if parallel_periods:
        lines.append("# è·¨é¡¹ç›®å¹¶è¡Œå·¥ä½œæ—¶é—´æ®µ")
        total_parallel_minutes = sum(p['duration_minutes'] for p in parallel_periods)
        lines.append(f"æ£€æµ‹åˆ° {len(parallel_periods)} ä¸ªå¹¶è¡Œå·¥ä½œæ—¶æ®µï¼Œæ€»é‡å æ—¶é•¿çº¦ {total_parallel_minutes} åˆ†é’Ÿ")
        for idx, p in enumerate(parallel_periods, 1):
            repos_str = ', '.join(p['repos'])
            lines.append(f"- å¹¶è¡Œæ—¶æ®µ{idx}: {p['start']} ~ {p['end']} ({p['duration_minutes']} åˆ†é’Ÿ, æ¶‰åŠé¡¹ç›®: {repos_str})")
        lines.append("")
    
    # å„é¡¹ç›®è¯¦ç»†ç»Ÿè®¡
    for repo_name, grouped in repo_to_grouped.items():
        if len(grouped) ==0:
            continue
        lines.append(f"\n# é¡¹ç›®ï¼š{repo_name}")
        sessions = repo_to_sessions[repo_name]
        if sessions:
            total_minutes = sum(s['duration_minutes'] for s in sessions)
            lines.append(f"å·¥ä½œä¼šè¯: {len(sessions)} ä¸ªï¼Œæ€»æ—¶é•¿çº¦ {total_minutes} åˆ†é’Ÿ")
            for idx, s in enumerate(sessions, 1):
                # æ ‡è®°æ˜¯å¦ä¸ºå¹¶è¡Œæ—¶æ®µ
                is_parallel = any(
                    not (s['end'] < pp['start'] or s['start'] > pp['end'])
                    for pp in parallel_periods
                    if repo_name in pp['repos']
                )
                parallel_marker = " [å¹¶è¡Œ]" if is_parallel else ""
                lines.append(f"- ä¼šè¯{idx}: {s['start']} ~ {s['end']} ({s['duration_minutes']} åˆ†é’Ÿ, {len(s['commits'])} æ¬¡æäº¤){parallel_marker}")
        # Feature windows
        flat_commits: List[Dict] = []
        for items in grouped.values():
            flat_commits.extend(items)
        fw = compute_feature_windows(flat_commits)
        if fw:
            lines.append("åŠŸèƒ½çª—å£:")
            for k, v in fw.items():
                duration = int((v['end'] - v['start']).total_seconds() // 60)
                lines.append(f"- {k}: {v['start']} ~ {v['end']} ({duration} åˆ†é’Ÿ, {v['count']} æ¬¡æäº¤)")
        for day, items in grouped.items():
            lines.append(f"\n## {day} ({len(items)} commits)")
            for c in items:
                sha = c['sha']
                files, ins, dels, body = repo_to_details[repo_name].get(sha, ([], 0, 0, ""))
                short_sha = sha[:8]
                time_part = ' '.join(c['date'].split(' ')[1:3]) if ' ' in c['date'] else c['date']
                lines.append(f"\n- [{short_sha}] {time_part}")
                lines.append(f"  æäº¤ä¿¡æ¯: {c['message']}")
                lines.append(f"  ç»Ÿè®¡: {ins} è¡Œæ–°å¢, {dels} è¡Œåˆ é™¤, {len(files)} ä¸ªæ–‡ä»¶")
                if body and body.strip() != c['message']:
                    lines.append(f"  è¯¦ç»†å†…å®¹:\n{body}")
                if files:
                    lines.append(f"  ä¿®æ”¹çš„æ–‡ä»¶: {', '.join(files[:20])}{' ...' if len(files) > 20 else ''}")
    return "\n".join(lines)

def generate_summary_with_openai(
    grouped: Dict[str, List[Dict]], 
    details: Dict[str, Tuple[List[str], int, int, str]],
    system_prompt: Optional[str] = None,
    openai_api_key: Optional[str] = None,
    model: str = "gpt-4o-mini",
    author: Optional[str] = None,
    gap_minutes: int = 60,
    repo_to_pull_times: Optional[Dict[str, List[datetime]]] = None
) -> str:
    """
    ä½¿ç”¨ OpenAI API ç”Ÿæˆå·¥ä½œæ€»ç»“ã€‚
    å¦‚æœæ²¡æœ‰æä¾› API keyï¼Œä¼šå°è¯•ä»ç¯å¢ƒå˜é‡ OPENAI_API_KEY è·å–ã€‚
    """
    if not OPENAI_AVAILABLE:
        return "é”™è¯¯ï¼šæœªå®‰è£… openai åŒ…ã€‚è¯·è¿è¡Œ: pip install openai"
    
    api_key = openai_api_key or os.getenv("OPENAI_API_KEY")
    if not api_key:
        return "é”™è¯¯ï¼šæœªæä¾› OpenAI API keyã€‚è¯·è®¾ç½®ç¯å¢ƒå˜é‡ OPENAI_API_KEY æˆ–ä½¿ç”¨ --openai-key å‚æ•°"
    
    client = OpenAI(api_key=api_key)
    
    # å…¼å®¹å•é¡¹ç›®æˆ–å¤šé¡¹ç›®ä¸Šä¸‹æ–‡
    if isinstance(grouped, dict) and grouped and all(isinstance(v, dict) for v in grouped.values()):
        # å¤šé¡¹ç›®ï¼šgrouped: repo -> {day -> commits}
        repo_to_grouped = grouped  # type: ignore
        repo_to_details = details  # type: ignore
        # ä½¿ç”¨ä¼ å…¥çš„ repo_to_pull_timesï¼ˆå¦‚æœæä¾›ï¼‰
        commit_context = build_commit_context_by_project(repo_to_grouped, repo_to_details, gap_minutes, repo_to_pull_times)  # type: ignore
    else:
        context_lines = []
        for day, items in grouped.items():
            context_lines.append(f"\n## {day} ({len(items)} commits)")
            for c in items:
                sha = c['sha']
                files, ins, dels, body = details.get(sha, ([], 0, 0, ""))
                short_sha = sha[:8]
                time_part = ' '.join(c['date'].split(' ')[1:3]) if ' ' in c['date'] else c['date']
                context_lines.append(f"\n- [{short_sha}] {time_part}")
                context_lines.append(f"  æäº¤ä¿¡æ¯: {c['message']}")
                context_lines.append(f"  ç»Ÿè®¡: {ins} è¡Œæ–°å¢, {dels} è¡Œåˆ é™¤, {len(files)} ä¸ªæ–‡ä»¶")
                if body and body.strip() != c['message']:
                    context_lines.append(f"  è¯¦ç»†å†…å®¹:\n{body}")
                if files:
                    context_lines.append(f"  ä¿®æ”¹çš„æ–‡ä»¶: {', '.join(files[:20])}{' ...' if len(files) > 20 else ''}")
        commit_context = "\n".join(context_lines)
    if len(commit_context) <10:
        return "ä»Šå¤©æ— å·¥ä½œï¼Œæ— æ³•ç”Ÿæˆå·¥ä½œæ€»ç»“ã€‚"
    system_msg = system_prompt or default_system_prompt + "\næ­¤å¤–ï¼Œè¯·æŒ‰é¡¹ç›®åˆ†åˆ«ä¼°ç®—æŠ•å…¥æ—¶é—´ï¼ˆæ ¹æ®æäº¤æ—¶é—´å¯†åº¦ä¸è¿ç»­æ€§ï¼‰ï¼Œå¹¶ç»™å‡ºæ¯ä¸ªé¡¹ç›®çš„ä¸»è¦äº§å‡ºã€‚"
    if author:
        system_msg += f"\næ­¤å¤–ï¼Œè¯·åŸºäºä½œè€…å§“åæˆ–é‚®ç®±åŒ…å«â€œ{author}â€çš„æäº¤è¿›è¡Œå·¥ä½œæ€»ç»“ï¼Œå¹¶åœ¨æ‘˜è¦å¼€å¤´æ˜¾å¼æ ‡æ³¨ï¼šä½œè€…ï¼š{author}ã€‚"
        user_msg = f"è¯·æ ¹æ®ä»¥ä¸‹ commit è®°å½•ç”Ÿæˆ{author}å·¥ä½œæ€»ç»“ï¼š\n\n{commit_context}"
        user_msg += PEI
    else:
        user_msg = f"è¯·æ ¹æ®ä»¥ä¸‹ commit è®°å½•ç”Ÿæˆå·¥ä½œæ€»ç»“ï¼š\n\n{commit_context}"
    
    try:
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_msg},
                {"role": "user", "content": user_msg}
            ],
            temperature=0.3
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"é”™è¯¯ï¼šè°ƒç”¨ OpenAI API å¤±è´¥: {str(e)}"

def generate_summary_with_deepseek(
    grouped: Dict[str, List[Dict]],
    details: Dict[str, Tuple[List[str], int, int, str]],
    system_prompt: Optional[str] = None,
    deepseek_api_key: Optional[str] = None,
    model: str = "deepseek-chat",
    author: Optional[str] = None,
    gap_minutes: int = 60,
    repo_to_pull_times: Optional[Dict[str, List[datetime]]] = None
) -> str:
    """
    ä½¿ç”¨ DeepSeek API ç”Ÿæˆå·¥ä½œæ€»ç»“ï¼ˆOpenAI å…¼å®¹çš„ Chat Completions æ ¼å¼ï¼‰ã€‚
    """
    final_key = deepseek_api_key or os.getenv("DEEPSEEK_API_KEY")
    if not final_key:
        return "é”™è¯¯ï¼šæœªæä¾› DeepSeek API keyã€‚è¯·è®¾ç½®ç¯å¢ƒå˜é‡ DEEPSEEK_API_KEY æˆ–ä½¿ç”¨ --deepseek-key å‚æ•°"

    # æ„å»ºä¸Šä¸‹æ–‡ï¼ˆæ”¯æŒå¤šé¡¹ç›®ï¼‰
    if isinstance(grouped, dict) and grouped and all(isinstance(v, dict) for v in grouped.values()):
        # ä½¿ç”¨ä¼ å…¥çš„ repo_to_pull_timesï¼ˆå¦‚æœæä¾›ï¼‰
        commit_context = build_commit_context_by_project(grouped, details, gap_minutes, repo_to_pull_times)  # type: ignore
    else:
        context_lines = []
        for day, items in grouped.items():
            context_lines.append(f"\n## {day} ({len(items)} commits)")
            for c in items:
                sha = c['sha']
                files, ins, dels, body = details.get(sha, ([], 0, 0, ""))
                short_sha = sha[:8]
                time_part = ' '.join(c['date'].split(' ')[1:3]) if ' ' in c['date'] else c['date']
                context_lines.append(f"\n- [{short_sha}] {time_part}")
                context_lines.append(f"  æäº¤ä¿¡æ¯: {c['message']}")
                context_lines.append(f"  ç»Ÿè®¡: {ins} è¡Œæ–°å¢, {dels} è¡Œåˆ é™¤, {len(files)} ä¸ªæ–‡ä»¶")
                if body and body.strip() != c['message']:
                    context_lines.append(f"  è¯¦ç»†å†…å®¹:\n{body}")
                if files:
                    context_lines.append(f"  ä¿®æ”¹çš„æ–‡ä»¶: {', '.join(files[:20])}{' ...' if len(files) > 20 else ''}")
        commit_context = "\n".join(context_lines)
    if len(commit_context) <10:
        return "ä»Šå¤©æ— å·¥ä½œï¼Œæ— æ³•ç”Ÿæˆå·¥ä½œæ€»ç»“ã€‚"
    system_msg = system_prompt or default_system_prompt + "\næ­¤å¤–ï¼Œè¯·æŒ‰é¡¹ç›®åˆ†åˆ«ä¼°ç®—æŠ•å…¥æ—¶é—´ï¼ˆæ ¹æ®æäº¤æ—¶é—´å¯†åº¦ä¸è¿ç»­æ€§ï¼‰ï¼Œå¹¶ç»™å‡ºæ¯ä¸ªé¡¹ç›®çš„ä¸»è¦äº§å‡ºã€‚"
    if author:
        system_msg += f"\næ­¤å¤–ï¼Œè¯·åŸºäºä½œè€…å§“åæˆ–é‚®ç®±åŒ…å«â€œ{author}â€çš„æäº¤è¿›è¡Œå·¥ä½œæ€»ç»“ï¼Œå¹¶åœ¨æ‘˜è¦å¼€å¤´æ˜¾å¼æ ‡æ³¨ï¼šä½œè€…ï¼š{author}ã€‚"
        user_msg = f"è¯·æ ¹æ®ä»¥ä¸‹ commit è®°å½•ç”Ÿæˆ{author}å·¥ä½œæ€»ç»“ï¼š\n\n{commit_context}"
        user_msg += PEI
    else:
        user_msg = f"è¯·æ ¹æ®ä»¥ä¸‹ commit è®°å½•ç”Ÿæˆå·¥ä½œæ€»ç»“ï¼š\n\n{commit_context}"
    

    # æ˜ å°„æ¨¡å‹åç§°ï¼ˆDeepSeek çš„æ­£ç¡®æ¨¡å‹åç§°ï¼‰
    model_map = {
        "deepseek-chat": "deepseek-chat",
        "deepseek-reasoner": "deepseek-reasoner",
        "chat": "deepseek-chat"  # é»˜è®¤
    }
    actual_model = model_map.get(model.lower(), "deepseek-chat")

    try:
        url = "https://api.deepseek.com/v1/chat/completions"
        headers = {
            "Authorization": f"Bearer {final_key}",
            "Content-Type": "application/json"
        }
        payload = {
            "model": actual_model,
            "messages": [
                {"role": "system", "content": system_msg},
                {"role": "user", "content": user_msg}
            ],
            "temperature": 0.3
        }
        resp = requests.post(url, headers=headers, json=payload, timeout=60)
        resp.raise_for_status()
        data = resp.json()
        return data["choices"][0]["message"]["content"].strip()
    except requests.exceptions.HTTPError as e:
        error_detail = ""
        try:
            error_detail = f" - {resp.text}"
        except:
            pass
        return f"é”™è¯¯ï¼šè°ƒç”¨ DeepSeek API å¤±è´¥: {str(e)}{error_detail}"
    except Exception as e:
        return f"é”™è¯¯ï¼šè°ƒç”¨ DeepSeek API å¤±è´¥: {str(e)}"

def render_markdown_worklog(
    title: str, 
    grouped: Dict[str, List[Dict]], 
    details: Dict[str, Tuple[List[str], int, int, str]],
    add_summary: bool = False,
    summary_text: Optional[str] = None
) -> str:
    lines: List[str] = []
    lines.append(f"# {title}")
    lines.append("")
    total_commits = sum(len(v) for v in grouped.values())
    lines.append(f"æ€»è®¡ {total_commits} ä¸ªæäº¤")
    lines.append("")
    for day, items in grouped.items():
        lines.append(f"## {day} ({len(items)} commits)")
        lines.append("")
        for c in items:
            sha = c['sha']
            short_sha = sha[:8]
            files, ins, dels, body = details.get(sha, ([], 0, 0, ""))
            time_part = ' '.join(c['date'].split(' ')[1:3]) if ' ' in c['date'] else c['date']
            lines.append(f"- [{short_sha}] {time_part} | {c['message']} ({ins}+/{dels}-; {len(files)} files)")
            if files:
                lines.append(f"  - files: {', '.join(files[:10])}{' ...' if len(files) > 10 else ''}")
            if body:
                lines.append("  - message:")
                lines.append("```")
                lines.extend(body.splitlines())
                lines.append("```")
        lines.append("")
    
    # æ·»åŠ æ€»ç»“
    if add_summary and summary_text:
        lines.append(summary_text)
    
    return "\n".join(lines)

def render_multi_project_worklog(title: str, repo_to_grouped: Dict[str, Dict[str, List[Dict]]], repo_to_details: Dict[str, Dict[str, Tuple[List[str], int, int, str]]], add_summary: bool = False, summary_text: Optional[str] = None, gap_minutes: int = 60, repo_to_pull_times: Optional[Dict[str, List[datetime]]] = None) -> str:
    lines: List[str] = []
    lines.append(f"# {title}")
    lines.append("")
    total_commits = sum(sum(len(v) for v in grouped.values()) for grouped in repo_to_grouped.values())
    lines.append(f"æ€»è®¡ {total_commits} ä¸ªæäº¤ï¼Œé¡¹ç›®æ•° {len(repo_to_grouped)}")
    lines.append("")
    
    # è®¡ç®—å¹¶è¡Œå·¥ä½œæ—¶é—´
    repo_to_sessions: Dict[str, List[Dict]] = {}
    for repo_name, grouped in repo_to_grouped.items():
        flat_commits: List[Dict] = []
        for items in grouped.values():
            flat_commits.extend(items)
        # è·å–è¯¥ä»“åº“çš„ pull æ—¶é—´ï¼ˆå¦‚æœæ˜¯æœ¬åœ°ä»“åº“ï¼‰
        pull_times = repo_to_pull_times.get(repo_name, []) if repo_to_pull_times else []
        sessions = compute_work_sessions(flat_commits, gap_minutes, pull_times)
        repo_to_sessions[repo_name] = sessions
    
    parallel_periods = detect_parallel_sessions(repo_to_sessions)
    if parallel_periods:
        lines.append("## è·¨é¡¹ç›®å¹¶è¡Œå·¥ä½œæ—¶é—´ç»Ÿè®¡")
        total_parallel_minutes = sum(p['duration_minutes'] for p in parallel_periods)
        lines.append(f"æ£€æµ‹åˆ° **{len(parallel_periods)} ä¸ªå¹¶è¡Œå·¥ä½œæ—¶æ®µ**ï¼Œæ€»é‡å æ—¶é•¿çº¦ **{total_parallel_minutes} åˆ†é’Ÿ**")
        lines.append("")
        for idx, p in enumerate(parallel_periods, 1):
            repos_str = ', '.join(p['repos'])
            lines.append(f"- **å¹¶è¡Œæ—¶æ®µ {idx}**ï¼š{p['start'].strftime('%Y-%m-%d %H:%M')} ~ {p['end'].strftime('%Y-%m-%d %H:%M')} ({p['duration_minutes']} åˆ†é’Ÿ)")
            lines.append(f"  - æ¶‰åŠé¡¹ç›®ï¼š{repos_str}")
        lines.append("")
        lines.append("> æ³¨æ„ï¼šå¹¶è¡Œå·¥ä½œæ—¶é—´ä¸åº”ç®€å•ç´¯åŠ ï¼Œå®é™…æŠ•å…¥æ—¶é—´ä»¥é‡å æ—¶æ®µçš„æœ€å¤§å€¼ä¸ºå‡†ã€‚")
        lines.append("")
    
    # å„é¡¹ç›®æ—¶é—´ç»Ÿè®¡
    lines.append("## å„é¡¹ç›®æ—¶é—´ç»Ÿè®¡")
    for repo_name, grouped in repo_to_grouped.items():
        sessions = repo_to_sessions[repo_name]
        if sessions:
            total_minutes = sum(s['duration_minutes'] for s in sessions)
            lines.append(f"### {repo_name}")
            lines.append(f"- å·¥ä½œä¼šè¯ï¼š{len(sessions)} ä¸ªï¼Œæ€»æ—¶é•¿çº¦ {total_minutes} åˆ†é’Ÿ")
            for idx, s in enumerate(sessions, 1):
                is_parallel = any(
                    not (s['end'] < pp['start'] or s['start'] > pp['end'])
                    for pp in parallel_periods
                    if repo_name in pp['repos']
                )
                parallel_marker = " **[å¹¶è¡Œ]**" if is_parallel else ""
                lines.append(f"  - ä¼šè¯{idx}ï¼š{s['start'].strftime('%H:%M')} ~ {s['end'].strftime('%H:%M')} ({s['duration_minutes']} åˆ†é’Ÿ, {len(s['commits'])} æ¬¡æäº¤){parallel_marker}")
    lines.append("")
    for repo_name, grouped in repo_to_grouped.items():
        lines.append(f"# é¡¹ç›®ï¼š{repo_name}")
        lines.append("")
        for day, items in grouped.items():
            lines.append(f"## {day} ({len(items)} commits)")
            lines.append("")
            for c in items:
                sha = c['sha']
                short_sha = sha[:8]
                files, ins, dels, body = repo_to_details[repo_name].get(sha, ([], 0, 0, ""))
                time_part = ' '.join(c['date'].split(' ')[1:3]) if ' ' in c['date'] else c['date']
                lines.append(f"- [{short_sha}] {time_part} | {c['message']} ({ins}+/{dels}-; {len(files)} files)")
                if files:
                    lines.append(f"  - files: {', '.join(files[:10])}{' ...' if len(files) > 10 else ''}")
                if body:
                    lines.append("  - message:")
                    lines.append("```")
                    lines.extend(body.splitlines())
                    lines.append("```")
            lines.append("")
        lines.append("")
    if add_summary and summary_text:
        lines.append(summary_text)
    return "\n".join(lines)

def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Generate work log from git commits")
    parser.add_argument('--repo', type=str, default=None, help='Path to git repository (single)')
    parser.add_argument('--repos', type=str, default=None, help='Multiple repositories, comma-separated')
    parser.add_argument('--github', type=str, default=None, help='GitHub repository (format: OWNER/REPO, comma-separated for multiple)')
    parser.add_argument('--gitee', type=str, default=None, help='Gitee repository (format: OWNER/REPO, comma-separated for multiple)')
    parser.add_argument('--github-token', type=str, default=None, help='GitHub token (or set GITHUB_TOKEN env var)')
    parser.add_argument('--gitee-token', type=str, default=None, help='Gitee token (or set GITEE_TOKEN env var)')
    parser.add_argument('--since', type=str, default=None, help='Start datetime (ISO or YYYY-MM-DD)')
    parser.add_argument('--until', type=str, default=None, help='End datetime (ISO or YYYY-MM-DD)')
    parser.add_argument('--days', type=int, default=None, help='If set, use last N days ending today')
    parser.add_argument('--session-gap-minutes', type=int, default=60, help='Gap minutes to split work sessions')
    parser.add_argument('--author', type=str, default=None, help='Filter by author name or email (optional)')
    parser.add_argument('--output', type=str, default=None, help='Output file path (.md). If not set, print to stdout')
    parser.add_argument('--title', type=str, default=None, help='Title for the work log document')
    parser.add_argument('--add-summary', action='store_true', help='Add AI-generated Chinese summary at the end')
    parser.add_argument('--openai-key', type=str, default=None, help='OpenAI API key (or set OPENAI_API_KEY env var)')
    parser.add_argument('--openai-model', type=str, default='gpt-4o-mini', help='OpenAI model to use (default: gpt-4o-mini)')
    parser.add_argument('--system-prompt-file', type=str, default=None, help='Path to custom system prompt file')
    # DeepSeek æ”¯æŒ
    parser.add_argument('--provider', type=str, default='openai', choices=['openai', 'deepseek'], help='LLM provider')
    parser.add_argument('--deepseek-key', type=str, default=None, help='DeepSeek API key (or set DEEPSEEK_API_KEY env var)')
    parser.add_argument('--deepseek-model', type=str, default='deepseek-chat', help='DeepSeek model (e.g., deepseek-chat, deepseek-reasoner)')
    return parser.parse_args()

def parse_date_input(value: Optional[str], default_dt: Optional[datetime]) -> Optional[datetime]:
    if value is None:
        return default_dt
    # try ISO then YYYY-MM-DD
    try:
        return datetime.fromisoformat(value)
    except Exception:
        try:
            return datetime.strptime(value, "%Y-%m-%d")
        except Exception:
            raise ValueError(f"æ— æ³•è§£ææ—¥æœŸ: {value}")

def git2work():
    args = parse_args()
    repo_paths: List[str] = []
    github_repos: List[str] = []
    gitee_repos: List[str] = []
    
    # è§£ææœ¬åœ°ä»“åº“
    if args.repos:
        repo_paths = [p.strip() for p in args.repos.split(',') if p.strip()]
    elif args.repo:
        repo_paths = [args.repo]
    
    # è§£æ GitHub ä»“åº“
    if args.github:
        github_repos = [r.strip() for r in args.github.split(',') if r.strip()]
    
    # è§£æ Gitee ä»“åº“
    if args.gitee:
        gitee_repos = [r.strip() for r in args.gitee.split(',') if r.strip()]
    
    # å¦‚æœæ²¡æœ‰ä»»ä½•ä»“åº“æŒ‡å®šï¼Œä½¿ç”¨é»˜è®¤æœ¬åœ°ä»“åº“
    if not repo_paths and not github_repos and not gitee_repos:
        repo_paths = ["/mnt/d/works/RayTracy"]

    now = datetime.now()
    if args.days is not None and args.days > 0:
        start = (now - timedelta(days=args.days - 1)).replace(hour=0, minute=0, second=0, microsecond=0)
        end = now.replace(hour=23, minute=59, second=59, microsecond=0)
    else:
        start = parse_date_input(args.since, now.replace(hour=0, minute=0, second=0, microsecond=0))
        end = parse_date_input(args.until, now.replace(hour=23, minute=59, second=59, microsecond=0))
        if start is not None:
            start = start.replace(hour=0, minute=0, second=0, microsecond=0)
        if end is not None:
            end = end.replace(hour=23, minute=59, second=59, microsecond=0)

    # è·å– GitHub token
    github_token = args.github_token or GITHUB_TOKEN
    if github_repos and not github_token:
        print("Warning: GitHub ä»“åº“éœ€è¦ tokenï¼Œè¯·è®¾ç½® --github-token æˆ–ç¯å¢ƒå˜é‡ GITHUB_TOKEN")
        github_repos = []  # è·³è¿‡ GitHub ä»“åº“
    
    # è·å– Gitee token
    gitee_token = args.gitee_token or GITEE_TOKEN
    if gitee_repos and not gitee_token:
        print("Warning: Gitee ä»“åº“éœ€è¦ tokenï¼Œè¯·è®¾ç½® --gitee-token æˆ–ç¯å¢ƒå˜é‡ GITEE_TOKEN")
        gitee_repos = []  # è·³è¿‡ Gitee ä»“åº“

    # è®¡ç®—å®é™…å¯ç”¨çš„ä»“åº“æ€»æ•°
    total_repos = len(repo_paths) + len(github_repos) + len(gitee_repos)
    # å¦‚æœä»»ä½•ä¸€ä¸ªç±»å‹æœ‰å¤šä¸ªä»“åº“ï¼Œæˆ–è€…æ€»ä»“åº“æ•°å¤§äº1ï¼Œéƒ½è¿›å…¥å¤šé¡¹ç›®æ¨¡å¼
    multi_project = (len(repo_paths) > 1 or len(github_repos) > 1 or len(gitee_repos) > 1 or total_repos > 1)

    if not multi_project:
        # å•é¡¹ç›®æ¨¡å¼ï¼ˆåªæœ‰å•ä¸ªä»“åº“æˆ–å•ä¸ªæ¥è‡ªä¸åŒä½ç½®çš„ä»“åº“ï¼‰
        commits: List[Dict] = []
        details: Dict[str, Tuple[List[str], int, int, str]] = {}
        pull_times: List[datetime] = []  # ç”¨äºå•é¡¹ç›®æ¨¡å¼çš„ pull æ—¶é—´
        
        # å¤„ç†æœ¬åœ°ä»“åº“ï¼ˆæœ€å¤šä¸€ä¸ªï¼‰
        if repo_paths:
            repo = repo_paths[0]
            commits = get_commits_between(repo, start, end)
            if args.author:
                author_lower = args.author.lower()
                commits = [c for c in commits if author_lower in c['author_name'].lower() or author_lower in c['author_email'].lower()]
            # è·å– pull æ“ä½œæ—¶é—´
            pull_times = get_pull_operations(repo, start, end)
            for c in commits:
                files, ins, dels = get_commit_numstat(repo, c['sha'])
                body = get_commit_body(repo, c['sha'])
                details[c['sha']] = (files, ins, dels, body)
        
        # å¤„ç† GitHub ä»“åº“ï¼ˆæœ€å¤šä¸€ä¸ªï¼‰
        if github_repos and github_token:
            repo_name = github_repos[0]
            try:
                remote_commits = get_github_events(repo_name, github_token, start, end)
                if args.author:
                    author_lower = args.author.lower()
                    remote_commits = [c for c in remote_commits if author_lower in c['author_name'].lower()]
                commits.extend(remote_commits)
                # è¿œç¨‹ä»“åº“æ— æ³•è·å– numstatï¼Œä½¿ç”¨å ä½å€¼
                for c in remote_commits:
                    details[c['sha']] = ([], 0, 0, c['message'])
            except Exception as e:
                print(f"Error: è·å– GitHub ä»“åº“ {repo_name} å¤±è´¥: {e}")
        
        # å¤„ç† Gitee ä»“åº“ï¼ˆæœ€å¤šä¸€ä¸ªï¼‰
        if gitee_repos and gitee_token:
            repo_name = gitee_repos[0]
            try:
                remote_commits = get_gitee_events(repo_name, gitee_token, start, end)
                if args.author:
                    author_lower = args.author.lower()
                    remote_commits = [c for c in remote_commits if author_lower in c['author_name'].lower()]
                commits.extend(remote_commits)
                # è¿œç¨‹ä»“åº“æ— æ³•è·å– numstatï¼Œä½¿ç”¨å ä½å€¼
                for c in remote_commits:
                    details[c['sha']] = ([], 0, 0, c['message'])
            except Exception as e:
                print(f"Error: è·å– Gitee ä»“åº“ {repo_name} å¤±è´¥: {e}")
        
        # æŒ‰æ—¶é—´æ’åºæ‰€æœ‰ commits
        commits.sort(key=lambda c: commit_time_dt(c))
        grouped = group_commits_by_date(commits)
        
        # ä¸ºå•é¡¹ç›®æ¨¡å¼åˆå§‹åŒ– repo_to_pull_times_multi
        # æ³¨æ„ï¼šå•é¡¹ç›®æ¨¡å¼ä¸æ˜¾ç¤ºä¼šè¯ç»Ÿè®¡ï¼Œä½†ä¿ç•™å˜é‡ä»¥ä¾¿ä¸€è‡´æ€§
        repo_to_pull_times_multi = None  # type: ignore
    else:
        # å¤šé¡¹ç›®æ¨¡å¼
        repo_to_commits: Dict[str, List[Dict]] = {}
        repo_to_details: Dict[str, Dict[str, Tuple[List[str], int, int, str]]] = {}
        repo_to_grouped: Dict[str, Dict[str, List[Dict]]] = {}
        repo_to_pull_times: Dict[str, List[datetime]] = {}  # å­˜å‚¨æ¯ä¸ªæœ¬åœ°ä»“åº“çš„ pull æ—¶é—´
        
        # å¤„ç†æœ¬åœ°ä»“åº“
        for repo in repo_paths:
            commits = get_commits_between(repo, start, end)
            if args.author:
                author_lower = args.author.lower()
                commits = [c for c in commits if author_lower in c['author_name'].lower() or author_lower in c['author_email'].lower()]
            # è·å– pull æ“ä½œæ—¶é—´ï¼ˆä»…æœ¬åœ°ä»“åº“ï¼‰
            pull_times = get_pull_operations(repo, start, end)
            repo_to_pull_times[repo] = pull_times
            repo_to_commits[repo] = commits
            details_map: Dict[str, Tuple[List[str], int, int, str]] = {}
            for c in commits:
                files, ins, dels = get_commit_numstat(repo, c['sha'])
                body = get_commit_body(repo, c['sha'])
                details_map[c['sha']] = (files, ins, dels, body)
            repo_to_details[repo] = details_map
            repo_to_grouped[repo] = group_commits_by_date(commits)
        
        # å¤„ç† GitHub ä»“åº“
        for repo_name in github_repos:
            if github_token:
                try:
                    commits = get_github_events(repo_name, github_token, start, end)
                    if args.author:
                        author_lower = args.author.lower()
                        commits = [c for c in commits if author_lower in c['author_name'].lower()]
                    repo_to_commits[repo_name] = commits
                    details_map: Dict[str, Tuple[List[str], int, int, str]] = {}
                    # è¿œç¨‹ä»“åº“æ— æ³•è·å– numstatï¼Œä½¿ç”¨å ä½å€¼
                    for c in commits:
                        details_map[c['sha']] = ([], 0, 0, c['message'])
                    repo_to_details[repo_name] = details_map
                    repo_to_grouped[repo_name] = group_commits_by_date(commits)
                except Exception as e:
                    print(f"Error: è·å– GitHub ä»“åº“ {repo_name} å¤±è´¥: {e}")
        
        # å¤„ç† Gitee ä»“åº“
        for repo_name in gitee_repos:
            if gitee_token:
                try:
                    commits = get_gitee_events(repo_name, gitee_token, start, end)
                    if args.author:
                        author_lower = args.author.lower()
                        commits = [c for c in commits if author_lower in c['author_name'].lower()]
                    repo_to_commits[repo_name] = commits
                    details_map: Dict[str, Tuple[List[str], int, int, str]] = {}
                    # è¿œç¨‹ä»“åº“æ— æ³•è·å– numstatï¼Œä½¿ç”¨å ä½å€¼
                    for c in commits:
                        details_map[c['sha']] = ([], 0, 0, c['message'])
                    repo_to_details[repo_name] = details_map
                    repo_to_grouped[repo_name] = group_commits_by_date(commits)
                except Exception as e:
                    print(f"Error: è·å– Gitee ä»“åº“ {repo_name} å¤±è´¥: {e}")
        
        grouped = repo_to_grouped  # type: ignore
        details = repo_to_details  # type: ignore
        # ä¿å­˜ repo_to_pull_times ä»¥ä¾¿åç»­ä½¿ç”¨
        repo_to_pull_times_multi = repo_to_pull_times  # type: ignore

    title = args.title or (f"Work Log: {start.date()} to {end.date()}" if start and end else "Work Log")
    
    # ç”Ÿæˆæ€»ç»“ï¼ˆå¦‚æœéœ€è¦ï¼‰
    summary_text = None
    if args.add_summary:
        print("æ­£åœ¨ç”Ÿæˆ AI æ€»ç»“...")
        # è¯»å–è‡ªå®šä¹‰æç¤ºè¯ï¼ˆå¦‚æœæœ‰ï¼‰
        system_prompt = None
        if args.system_prompt_file and os.path.exists(args.system_prompt_file):
            with open(args.system_prompt_file, 'r', encoding='utf-8') as f:
                system_prompt = f.read()
        
        # å‡†å¤‡ repo_to_pull_timesï¼ˆä»…åœ¨å¤šé¡¹ç›®æ¨¡å¼ä¸‹ä½¿ç”¨ï¼‰
        repo_to_pull_times_for_summary = repo_to_pull_times_multi if multi_project else None
        
        if getattr(args, 'provider', 'openai') == 'deepseek':
            summary_text = generate_summary_with_deepseek(
                grouped,  # type: ignore
                details,  # type: ignore
                system_prompt=system_prompt,
                deepseek_api_key=args.deepseek_key,
                model=args.deepseek_model,
                author=args.author,
                gap_minutes=args.session_gap_minutes,
                repo_to_pull_times=repo_to_pull_times_for_summary
            )
        else:
            summary_text = generate_summary_with_openai(
                grouped,  # type: ignore
                details,  # type: ignore
                system_prompt=system_prompt,
                openai_api_key=args.openai_key,
                model=args.openai_model,
                author=args.author,
                gap_minutes=args.session_gap_minutes,
                repo_to_pull_times=repo_to_pull_times_for_summary
            )
        print("AI æ€»ç»“ç”Ÿæˆå®Œæˆ")
    
    if not multi_project:
        md = render_markdown_worklog(title, grouped, details, add_summary=args.add_summary, summary_text=summary_text)  # type: ignore
    else:
        md = render_multi_project_worklog(title, grouped, details, add_summary=args.add_summary, summary_text=summary_text, gap_minutes=args.session_gap_minutes, repo_to_pull_times=repo_to_pull_times_multi)  # type: ignore

    if args.output:
        os.makedirs(os.path.dirname(args.output), exist_ok=True) if os.path.dirname(args.output) else None
        with open(args.output, 'w', encoding='utf-8') as f:
            f.write(md)
        print(f"å·²å†™å…¥: {args.output}")
    else:
        print(md)

if __name__ == "__main__":
    git2work()