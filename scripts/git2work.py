#! /usr/bin/env python3

import os
import sys
import subprocess
import argparse
import re
import json
import requests
from collections import defaultdict
from typing import Dict, List, Optional, Tuple
from git import Repo
from datetime import datetime, timedelta
# API Keys - ä»…ä»ç¯å¢ƒå˜é‡è¯»å–ï¼Œä¸æä¾›é»˜è®¤å€¼ä»¥ç¡®ä¿å®‰å…¨
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    print("Warning: openai package not installed. Please run: pip install openai")


# é»˜è®¤ç³»ç»Ÿæç¤ºè¯
default_system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ€æœ¯æ–‡æ¡£æ’°å†™åŠ©æ‰‹ã€‚æ ¹æ®æä¾›çš„ git commit è®°å½•ï¼Œç”Ÿæˆä¸€ä»½ç»“æ„åŒ–çš„ä¸­æ–‡å·¥ä½œæ€»ç»“ã€‚

è¦æ±‚ï¼š
1. ä½¿ç”¨ Markdown æ ¼å¼
2. æ€»ç»“ä¸»è¦åŒ…æ‹¬ï¼š
   - ä»Šæ—¥å·¥ä½œæ¦‚è¿°ï¼ˆ3-5å¥ï¼‰
   - ä¸»è¦å®Œæˆå†…å®¹ï¼ˆæŒ‰æ¨¡å—åˆ†ç±»ï¼‰
   - ç»Ÿè®¡æ•°æ®ï¼ˆæäº¤æ•°ã€ä»£ç å˜æ›´ã€æ¶‰åŠæ–‡ä»¶ç­‰ï¼‰
   - æŠ€æœ¯äº®ç‚¹æˆ–é‡è¦æ”¹è¿›
3. è¯­è¨€ç®€æ´ä¸“ä¸šï¼Œé¿å…è¿‡äºå†—é•¿
4. é‡ç‚¹å…³æ³¨ä»£ç æ”¹è¿›ã€åŠŸèƒ½å¢å¼ºã€é—®é¢˜ä¿®å¤ç­‰æŠ€æœ¯æ€§å†…å®¹

ã€å·¥ä½œä¼šè¯æ—¶é—´å›¾è¦æ±‚ã€‘
å¦‚æœæä¾›çš„ commit è®°å½•ä¸­åŒ…å«å·¥ä½œä¼šè¯ç»Ÿè®¡ä¿¡æ¯ï¼ˆå¦‚"å·¥ä½œä¼šè¯: X ä¸ªï¼Œæ€»æ—¶é•¿çº¦ Y åˆ†é’Ÿ"ï¼‰ï¼Œè¯·åŸºäºè¿™äº›ä¼šè¯ä¿¡æ¯ç»˜åˆ¶ä¸€ä¸ªç®€æ´çš„å·¥ä½œå†…å®¹æ—¶é—´åˆ†å¸ƒå›¾ã€‚å›¾åº”åŒ…å«ï¼š
1. å„å·¥ä½œä¼šè¯çš„èµ·æ­¢æ—¶é—´èŒƒå›´
2. æ¯ä¸ªä¼šè¯æ¶‰åŠçš„æäº¤æ•°é‡æˆ–ä¸»è¦åŠŸèƒ½æ¨¡å—
3. ä¼šè¯ä¹‹é—´çš„æ—¶é—´é—´éš”ï¼ˆä¾¿äºè¯†åˆ«å·¥ä½œèŠ‚å¥ï¼‰
4. å¦‚æœ‰è·¨é¡¹ç›®æäº¤ï¼Œæ ‡æ³¨é¡¹ç›®åˆ‡æ¢çš„æ—¶é—´ç‚¹
5. **ç‰¹åˆ«æ³¨æ„å¹¶è¡Œå·¥ä½œæ—¶é—´**ï¼šå¦‚æœå­˜åœ¨"è·¨é¡¹ç›®å¹¶è¡Œå·¥ä½œæ—¶é—´æ®µ"æ ‡è¯†ï¼Œè¯·åœ¨æ—¶é—´å›¾ä¸­æ¸…æ™°æ ‡æ³¨åŒæ—¶åœ¨ä¸åŒé¡¹ç›®ä¸Šå·¥ä½œçš„æ—¶æ®µï¼Œè¿™æœ‰åŠ©äºå‡†ç¡®è¯„ä¼°å®é™…æŠ•å…¥æ—¶é—´ï¼ˆå¹¶è¡Œå·¥ä½œä¸åº”ç®€å•ç´¯åŠ ï¼‰

æ—¶é—´å›¾å¯ä½¿ç”¨ Markdown è¡¨æ ¼åŠ Mermaid 10åˆ†é’Ÿçº§ç”˜ç‰¹å›¾å½¢å¼å‘ˆç°ã€‚

è¯·æ ¹æ®æä¾›çš„ commit ä¿¡æ¯ç”Ÿæˆå·¥ä½œæ€»ç»“ã€‚"""

def parse_git_log(raw):
    # æˆ‘ä»¬ä½¿ç”¨ git log è¾“å‡ºä»¥ \x1eï¼ˆrecord sepï¼‰åˆ†å‰² commitï¼Œä»¥ \x1f å­—æ®µåˆ†å‰²
    commits = []
    if not raw:
        return commits
    for entry in raw.strip("\x1e").split("\x1e"):
        parts = entry.split("\x1f")
        if len(parts) < 5:
            continue
        # æ”¯æŒä¸¤ç§æ ¼å¼ï¼š5å­—æ®µ(æ—§) æˆ– 6å­—æ®µ(å« %at epoch)
        if len(parts) >= 6:
            sha, author_name, author_email, date_str, epoch_str, message = [p.strip() for p in parts[:6]]
            date_epoch = int(epoch_str) if epoch_str.isdigit() else None
        else:
            sha, author_name, author_email, date_str, message = [p.strip() for p in parts[:5]]
            date_epoch = None
        # date_str ç¤ºä¾‹: 2025-10-20 12:34:56 +0800 ï¼ˆå–å†³äº --date=isoï¼‰
        commits.append({
            "sha": sha,
            "author_name": author_name,
            "author_email": author_email,
            "date": date_str,
            "date_epoch": date_epoch,
            "message": message,
        })
    return commits

def get_commits_between(repo_path, since_dt, until_dt, max_count=None):
    """
    since_dt / until_dt: python datetimeï¼ˆæœ€å¥½å¸¦æ—¶åŒºæˆ–è€…æ˜¯æœ¬åœ°æ—¶é—´ï¼‰
    è¿”å› commit å¯¹è±¡åˆ—è¡¨ï¼ˆå¯ä»¥è½¬æ¢ä¸º dictï¼‰
    """
    repo = Repo(repo_path)
    # gitpython æ²¡æœ‰ç›´æ¥å‚æ•°ç”¨æ¥ç­›é€‰æ—¥æœŸï¼Œæ‰€ä»¥ä½¿ç”¨ git directly via repo.git.log æ›´æ–¹ä¾¿ï¼š
    since = since_dt.isoformat(sep=' ')
    until = until_dt.isoformat(sep=' ')
    # å¢åŠ  %atï¼ˆauthor epoch ç§’ï¼‰ä¾¿äºç¨³å®šæ—¶é—´ç»Ÿè®¡
    raw = repo.git.log(
        f'--since={since}',
        f'--until={until}',
        '--pretty=format:%H%x1f%an%x1f%ae%x1f%ad%x1f%at%x1f%s%x1e',
        date='iso'
    )
    # å¤ç”¨ä¸Šé¢ parse å‡½æ•°
    return parse_git_log(raw)

def get_commit_numstat(repo_path: str, sha: str) -> Tuple[List[str], int, int]:
    """
    è¿”å› (files, insertions, deletions)
    é€šè¿‡ `git show --numstat` è§£ææ¯ä¸ª commit ä¿®æ”¹çš„æ–‡ä»¶ä¸å¢åˆ è¡Œæ•°ã€‚
    """
    repo = Repo(repo_path)
    # --pretty=tformat: åªè¾“å‡ºæ–‡ä»¶å˜æ›´ï¼ˆé¿å…é‡å¤å…ƒä¿¡æ¯ï¼‰
    output = repo.git.show(sha, '--numstat', '--pretty=tformat:')
    files: List[str] = []
    insertions_total = 0
    deletions_total = 0
    for line in output.splitlines():
        parts = line.split('\t')
        if len(parts) == 3:
            add_str, del_str, path = parts
            # äºŒè¿›åˆ¶æ–‡ä»¶ä¼šæ˜¾ç¤º '-'ï¼Œæ­¤æ—¶è®¡ä¸º 0
            try:
                add = int(add_str) if add_str.isdigit() else 0
            except ValueError:
                add = 0
            try:
                dele = int(del_str) if del_str.isdigit() else 0
            except ValueError:
                dele = 0
            insertions_total += add
            deletions_total += dele
            files.append(path)
    return files, insertions_total, deletions_total

def get_commit_body(repo_path: str, sha: str) -> str:
    """
    è·å–å®Œæ•´ commit messageï¼ˆå«ä¸»é¢˜ä¸æ­£æ–‡ï¼‰ã€‚
    """
    repo = Repo(repo_path)
    body = repo.git.show(sha, '-s', '--format=%B')
    return body.strip('\n')

def group_commits_by_date(commits: List[Dict]) -> Dict[str, List[Dict]]:
    groups: Dict[str, List[Dict]] = defaultdict(list)
    for c in commits:
        # date å­—ç¬¦ä¸²å½¢å¦‚ "2025-10-20 12:34:56 +0800"
        date_part = c['date'].split(' ')[0]
        groups[date_part].append(c)
    # å¯¹æ¯ç»„æŒ‰æ—¶é—´æ’åºï¼ˆä»æ—©åˆ°æ™šï¼‰
    for k in groups:
        groups[k].sort(key=lambda x: x['date'])
    return dict(sorted(groups.items(), key=lambda x: x[0]))

def commit_time_dt(c: Dict) -> datetime:
    if c.get('date_epoch'):
        try:
            return datetime.fromtimestamp(int(c['date_epoch']))
        except Exception:
            pass
    # Fallback parse from string
    ds = c.get('date', '')
    try:
        return datetime.strptime(ds, "%Y-%m-%d %H:%M:%S %z").astimezone().replace(tzinfo=None)
    except Exception:
        try:
            return datetime.strptime(ds, "%Y-%m-%d %H:%M:%S")
        except Exception:
            return datetime.fromisoformat(ds.replace(' ', 'T').split(' +')[0])

def compute_work_sessions(commits: List[Dict], gap_minutes: int = 60) -> List[Dict]:
    if not commits:
        return []
    items = sorted(commits, key=lambda c: commit_time_dt(c))
    sessions: List[Dict] = []
    gap = timedelta(minutes=gap_minutes)
    current = {
        'start': commit_time_dt(items[0]),
        'end': commit_time_dt(items[0]),
        'commits': [items[0]],
    }
    for c in items[1:]:
        t = commit_time_dt(c)
        if t - commit_time_dt(current['commits'][-1]) <= gap:
            current['end'] = t
            current['commits'].append(c)
        else:
            current['duration_minutes'] = max(1, int((current['end'] - current['start']).total_seconds() // 60))
            sessions.append(current)
            current = {'start': t, 'end': t, 'commits': [c]}
    current['duration_minutes'] = max(1, int((current['end'] - current['start']).total_seconds() // 60))
    sessions.append(current)
    return sessions

def compute_feature_windows(commits: List[Dict]) -> Dict[str, Dict]:
    # Group by leading token of commit message (e.g., feat, fix, docs, build, refactor)
    windows: Dict[str, Dict] = {}
    for c in commits:
        msg = c.get('message', '').strip()
        token = msg.split(':', 1)[0].lower().split(' ')[0]
        key = token if token in ['feat', 'fix', 'docs', 'build', 'refactor', 'chore', 'perf', 'test'] else 'other'
        t = commit_time_dt(c)
        w = windows.get(key)
        if not w:
            windows[key] = {'start': t, 'end': t, 'count': 1}
        else:
            if t < w['start']:
                w['start'] = t
            if t > w['end']:
                w['end'] = t
            w['count'] += 1
    return windows

def detect_parallel_sessions(repo_to_sessions: Dict[str, List[Dict]]) -> List[Dict]:
    """
    æ£€æµ‹è·¨é¡¹ç›®çš„å¹¶è¡Œå·¥ä½œæ—¶æ®µã€‚
    è¿”å›é‡å çš„æ—¶é—´æ®µåŠå…¶æ¶‰åŠçš„é¡¹ç›®åˆ—è¡¨ã€‚
    """
    if len(repo_to_sessions) < 2:
        return []  # å•é¡¹ç›®ä¸éœ€è¦æ£€æµ‹å¹¶è¡Œ
    
    all_periods: List[Dict] = []
    for repo, sessions in repo_to_sessions.items():
        for s in sessions:
            all_periods.append({
                'start': s['start'],
                'end': s['end'],
                'repo': repo,
                'session': s
            })
    
    if not all_periods:
        return []
    
    # åˆå¹¶é‡å æ—¶æ®µç®—æ³•ï¼šæŒ‰æ—¶é—´çº¿æ‰«æï¼Œåˆå¹¶è¿ç»­æˆ–é‡å çš„æ—¶æ®µ
    all_periods.sort(key=lambda x: (x['start'], x['end']))
    merged_overlaps: List[Dict] = []
    
    current_overlaps = []  # å½“å‰æ­£åœ¨é‡å çš„æ—¶æ®µç»„
    
    for period in all_periods:
        if not current_overlaps:
            current_overlaps = [period]
            continue
        
        # æ£€æŸ¥å½“å‰æ—¶æ®µæ˜¯å¦ä¸å·²æœ‰é‡å ç»„æœ‰æ—¶é—´é‡å 
        can_merge = False
        for existing in current_overlaps:
            if not (period['end'] < existing['start'] or period['start'] > existing['end']):
                can_merge = True
                break
        
        if can_merge:
            current_overlaps.append(period)
        else:
            # ç»“æŸå½“å‰é‡å ç»„ï¼Œå¼€å§‹æ–°çš„
            if len(set(p['repo'] for p in current_overlaps)) > 1:
                overlap_start = min(p['start'] for p in current_overlaps)
                overlap_end = max(p['end'] for p in current_overlaps)
                overlap_repos = sorted(set(p['repo'] for p in current_overlaps))
                merged_overlaps.append({
                    'start': overlap_start,
                    'end': overlap_end,
                    'repos': overlap_repos,
                    'duration_minutes': int((overlap_end - overlap_start).total_seconds() // 60)
                })
            current_overlaps = [period]
    
    # å¤„ç†æœ€åä¸€ç»„
    if len(set(p['repo'] for p in current_overlaps)) > 1:
        overlap_start = min(p['start'] for p in current_overlaps)
        overlap_end = max(p['end'] for p in current_overlaps)
        overlap_repos = sorted(set(p['repo'] for p in current_overlaps))
        merged_overlaps.append({
            'start': overlap_start,
            'end': overlap_end,
            'repos': overlap_repos,
            'duration_minutes': int((overlap_end - overlap_start).total_seconds() // 60)
        })
    
    # å†æ¬¡åˆå¹¶å¯èƒ½è¿ç»­æˆ–éƒ¨åˆ†é‡å çš„æ—¶æ®µ
    if not merged_overlaps:
        return []
    
    final_merged: List[Dict] = []
    merged_overlaps.sort(key=lambda x: (x['start'], x['end']))
    
    current = merged_overlaps[0]
    for next_period in merged_overlaps[1:]:
        # å¦‚æœæ—¶é—´æœ‰é‡å æˆ–è¿ç»­ï¼ˆé—´éš”å°äº5åˆ†é’Ÿè§†ä¸ºè¿ç»­ï¼‰ï¼Œä¸”æ¶‰åŠç›¸åŒé¡¹ç›®ï¼Œåˆ™åˆå¹¶
        gap = (next_period['start'] - current['end']).total_seconds() / 60
        if gap <= 5 or not (next_period['end'] < current['start'] or next_period['start'] > current['end']):
            # åˆå¹¶
            current['start'] = min(current['start'], next_period['start'])
            current['end'] = max(current['end'], next_period['end'])
            current['repos'] = sorted(set(current['repos']) | set(next_period['repos']))
            current['duration_minutes'] = int((current['end'] - current['start']).total_seconds() // 60)
        else:
            final_merged.append(current)
            current = next_period
    final_merged.append(current)
    
    return final_merged

def build_commit_context_by_project(repo_to_grouped: Dict[str, Dict[str, List[Dict]]], repo_to_details: Dict[str, Dict[str, Tuple[List[str], int, int, str]]], gap_minutes: int = 60) -> str:
    lines: List[str] = []
    
    # å…ˆè®¡ç®—æ‰€æœ‰é¡¹ç›®çš„ä¼šè¯ï¼Œç”¨äºæ£€æµ‹å¹¶è¡Œå·¥ä½œ
    repo_to_sessions: Dict[str, List[Dict]] = {}
    for repo_name, grouped in repo_to_grouped.items():
        flat_commits: List[Dict] = []
        for items in grouped.values():
            flat_commits.extend(items)
        sessions = compute_work_sessions(flat_commits, gap_minutes)
        repo_to_sessions[repo_name] = sessions
    
    # æ£€æµ‹è·¨é¡¹ç›®å¹¶è¡Œå·¥ä½œæ—¶é—´
    parallel_periods = detect_parallel_sessions(repo_to_sessions)
    if parallel_periods:
        lines.append("# è·¨é¡¹ç›®å¹¶è¡Œå·¥ä½œæ—¶é—´æ®µ")
        total_parallel_minutes = sum(p['duration_minutes'] for p in parallel_periods)
        lines.append(f"æ£€æµ‹åˆ° {len(parallel_periods)} ä¸ªå¹¶è¡Œå·¥ä½œæ—¶æ®µï¼Œæ€»é‡å æ—¶é•¿çº¦ {total_parallel_minutes} åˆ†é’Ÿ")
        for idx, p in enumerate(parallel_periods, 1):
            repos_str = ', '.join(p['repos'])
            lines.append(f"- å¹¶è¡Œæ—¶æ®µ{idx}: {p['start']} ~ {p['end']} ({p['duration_minutes']} åˆ†é’Ÿ, æ¶‰åŠé¡¹ç›®: {repos_str})")
        lines.append("")
    
    # å„é¡¹ç›®è¯¦ç»†ç»Ÿè®¡
    for repo_name, grouped in repo_to_grouped.items():
        lines.append(f"\n# é¡¹ç›®ï¼š{repo_name}")
        sessions = repo_to_sessions[repo_name]
        if sessions:
            total_minutes = sum(s['duration_minutes'] for s in sessions)
            lines.append(f"å·¥ä½œä¼šè¯: {len(sessions)} ä¸ªï¼Œæ€»æ—¶é•¿çº¦ {total_minutes} åˆ†é’Ÿ")
            for idx, s in enumerate(sessions, 1):
                # æ ‡è®°æ˜¯å¦ä¸ºå¹¶è¡Œæ—¶æ®µ
                is_parallel = any(
                    not (s['end'] < pp['start'] or s['start'] > pp['end'])
                    for pp in parallel_periods
                    if repo_name in pp['repos']
                )
                parallel_marker = " [å¹¶è¡Œ]" if is_parallel else ""
                lines.append(f"- ä¼šè¯{idx}: {s['start']} ~ {s['end']} ({s['duration_minutes']} åˆ†é’Ÿ, {len(s['commits'])} æ¬¡æäº¤){parallel_marker}")
        # Feature windows
        flat_commits: List[Dict] = []
        for items in grouped.values():
            flat_commits.extend(items)
        fw = compute_feature_windows(flat_commits)
        if fw:
            lines.append("åŠŸèƒ½çª—å£:")
            for k, v in fw.items():
                duration = int((v['end'] - v['start']).total_seconds() // 60)
                lines.append(f"- {k}: {v['start']} ~ {v['end']} ({duration} åˆ†é’Ÿ, {v['count']} æ¬¡æäº¤)")
        for day, items in grouped.items():
            lines.append(f"\n## {day} ({len(items)} commits)")
            for c in items:
                sha = c['sha']
                files, ins, dels, body = repo_to_details[repo_name].get(sha, ([], 0, 0, ""))
                short_sha = sha[:8]
                time_part = ' '.join(c['date'].split(' ')[1:3]) if ' ' in c['date'] else c['date']
                lines.append(f"\n- [{short_sha}] {time_part}")
                lines.append(f"  æäº¤ä¿¡æ¯: {c['message']}")
                lines.append(f"  ç»Ÿè®¡: {ins} è¡Œæ–°å¢, {dels} è¡Œåˆ é™¤, {len(files)} ä¸ªæ–‡ä»¶")
                if body and body.strip() != c['message']:
                    lines.append(f"  è¯¦ç»†å†…å®¹:\n{body}")
                if files:
                    lines.append(f"  ä¿®æ”¹çš„æ–‡ä»¶: {', '.join(files[:20])}{' ...' if len(files) > 20 else ''}")
    return "\n".join(lines)

def generate_summary_with_openai(
    grouped: Dict[str, List[Dict]], 
    details: Dict[str, Tuple[List[str], int, int, str]],
    system_prompt: Optional[str] = None,
    openai_api_key: Optional[str] = None,
    model: str = "gpt-4o-mini",
    author: Optional[str] = None,
    gap_minutes: int = 60
) -> str:
    """
    ä½¿ç”¨ OpenAI API ç”Ÿæˆå·¥ä½œæ€»ç»“ã€‚
    å¦‚æœæ²¡æœ‰æä¾› API keyï¼Œä¼šå°è¯•ä»ç¯å¢ƒå˜é‡ OPENAI_API_KEY è·å–ã€‚
    """
    if not OPENAI_AVAILABLE:
        return "é”™è¯¯ï¼šæœªå®‰è£… openai åŒ…ã€‚è¯·è¿è¡Œ: pip install openai"
    
    api_key = openai_api_key or os.getenv("OPENAI_API_KEY")
    if not api_key:
        return "é”™è¯¯ï¼šæœªæä¾› OpenAI API keyã€‚è¯·è®¾ç½®ç¯å¢ƒå˜é‡ OPENAI_API_KEY æˆ–ä½¿ç”¨ --openai-key å‚æ•°"
    
    client = OpenAI(api_key=api_key)
    
    # å…¼å®¹å•é¡¹ç›®æˆ–å¤šé¡¹ç›®ä¸Šä¸‹æ–‡
    if isinstance(grouped, dict) and grouped and all(isinstance(v, dict) for v in grouped.values()):
        # å¤šé¡¹ç›®ï¼šgrouped: repo -> {day -> commits}
        repo_to_grouped = grouped  # type: ignore
        repo_to_details = details  # type: ignore
        commit_context = build_commit_context_by_project(repo_to_grouped, repo_to_details, gap_minutes)  # type: ignore
    else:
        context_lines = []
        for day, items in grouped.items():
            context_lines.append(f"\n## {day} ({len(items)} commits)")
            for c in items:
                sha = c['sha']
                files, ins, dels, body = details.get(sha, ([], 0, 0, ""))
                short_sha = sha[:8]
                time_part = ' '.join(c['date'].split(' ')[1:3]) if ' ' in c['date'] else c['date']
                context_lines.append(f"\n- [{short_sha}] {time_part}")
                context_lines.append(f"  æäº¤ä¿¡æ¯: {c['message']}")
                context_lines.append(f"  ç»Ÿè®¡: {ins} è¡Œæ–°å¢, {dels} è¡Œåˆ é™¤, {len(files)} ä¸ªæ–‡ä»¶")
                if body and body.strip() != c['message']:
                    context_lines.append(f"  è¯¦ç»†å†…å®¹:\n{body}")
                if files:
                    context_lines.append(f"  ä¿®æ”¹çš„æ–‡ä»¶: {', '.join(files[:20])}{' ...' if len(files) > 20 else ''}")
        commit_context = "\n".join(context_lines)

    system_msg = system_prompt or default_system_prompt + "\næ­¤å¤–ï¼Œè¯·æŒ‰é¡¹ç›®åˆ†åˆ«ä¼°ç®—æŠ•å…¥æ—¶é—´ï¼ˆæ ¹æ®æäº¤æ—¶é—´å¯†åº¦ä¸è¿ç»­æ€§ï¼‰ï¼Œå¹¶ç»™å‡ºæ¯ä¸ªé¡¹ç›®çš„ä¸»è¦äº§å‡ºã€‚"
    if author:
        system_msg += f"\næ­¤å¤–ï¼Œè¯·åŸºäºä½œè€…å§“åæˆ–é‚®ç®±åŒ…å«â€œ{author}â€çš„æäº¤è¿›è¡Œå·¥ä½œæ€»ç»“ï¼Œå¹¶åœ¨æ‘˜è¦å¼€å¤´æ˜¾å¼æ ‡æ³¨ï¼šä½œè€…ï¼š{author}ã€‚"
        user_msg = f"è¯·æ ¹æ®ä»¥ä¸‹ commit è®°å½•ç”Ÿæˆ{author}å·¥ä½œæ€»ç»“ï¼š\n\n{commit_context}"
        user_msg += """\n\næœ€åè®¡ç®—ä¸€ä¸‹æ•ˆç‡æŒ‡æ•°ï¼ˆPEIï¼‰ï¼š
        è®¾ï¼š
* $N_c$ = å½“æ—¥æäº¤æ¬¡æ•°
* $L_{add}$ = æ–°å¢ä»£ç è¡Œæ•°
* $L_{del}$ = åˆ é™¤ä»£ç è¡Œæ•°
* $T$ = å®é™…æŠ•å…¥æ—¶é—´ï¼ˆå°æ—¶ï¼Œæ’é™¤å¹¶è¡Œé‡å ï¼‰
* $P_{mod}$ = ä¿®æ”¹æ–‡ä»¶æ•°
* $C_{eff}$ = ç¼–è¯‘é€šè¿‡ç‡ï¼ˆæˆ–æµ‹è¯•é€šè¿‡ç‡ï¼Œ0~1ï¼‰
* $C_{cmp}$ = ä»£ç å¤æ‚åº¦ç³»æ•°ï¼ˆ0.5~1.5ï¼Œå¯ä¾æ®ä»»åŠ¡ç±»å‹è°ƒæ•´ï¼‰
---
å…¬å¼ï¼š
$$
\\text{PEI} = \\frac{(0.4 N_c + 0.3 \\log_{10}(L_{add}+L_{del}) + 0.2 \\log_{10}(P_{mod}+1)) \\times C_{eff} \\times C_{cmp}}{T/8}
$$
> è¯´æ˜ï¼š
>
> * å¯¹æ•°é¡¹ä½¿å¾—ä»£ç é‡å’Œæ–‡ä»¶æ•°å¸¦æ¥é€’å‡æ•ˆç›Šï¼Œé˜²æ­¢è¡Œæ•°å †ç§¯é€ æˆè™šé«˜ã€‚
> * $T/8$ ç”¨äºæ—¶é—´å½’ä¸€åŒ–ï¼ˆä»¥ 8 å°æ—¶ä¸ºæ ‡å‡†å·¥ä½œæ—¥ï¼‰ã€‚
> * ç³»æ•°å¯è°ƒï¼š`0.4/0.3/0.2` æƒé‡é€‚åˆä¸­å‹é¡¹ç›®ï¼ˆå¦‚C++å·¥ç¨‹ï¼‰ã€‚
å‚è€ƒè§£é‡Šè¡¨

| PEI å€¼ | æ•ˆç‡ç­‰çº§  | ç‰¹å¾æè¿°           |
| ----- | ----- | -------------- |
| 0â€“3   | ğŸ’¤ ä½æ•ˆ | é¢‘ç¹ä¸Šä¸‹æ–‡åˆ‡æ¢ã€éæ ¸å¿ƒä»»åŠ¡  |
| 4â€“6   | âš™ï¸ æ­£å¸¸ | æŒç»­æ¨è¿›ã€ç¨³å®šäº§å‡º      |
| 7â€“9   | ğŸš€ é«˜æ•ˆ | æ¨¡å—é‡æ„ã€ç³»ç»Ÿä¼˜åŒ–æˆ–å…³é”®ä¿®å¤ |
| â‰¥10   | ğŸ§  å“è¶Š | è‡ªåŠ¨åŒ–ã€ç”Ÿæˆå¼ä»»åŠ¡ã€é›†ä¸­æ”»åš |
        """
    else:
        user_msg = f"è¯·æ ¹æ®ä»¥ä¸‹ commit è®°å½•ç”Ÿæˆå·¥ä½œæ€»ç»“ï¼š\n\n{commit_context}"
    
    try:
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_msg},
                {"role": "user", "content": user_msg}
            ],
            temperature=0.3
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"é”™è¯¯ï¼šè°ƒç”¨ OpenAI API å¤±è´¥: {str(e)}"

def generate_summary_with_deepseek(
    grouped: Dict[str, List[Dict]],
    details: Dict[str, Tuple[List[str], int, int, str]],
    system_prompt: Optional[str] = None,
    deepseek_api_key: Optional[str] = None,
    model: str = "deepseek-chat",
    author: Optional[str] = None,
    gap_minutes: int = 60
) -> str:
    """
    ä½¿ç”¨ DeepSeek API ç”Ÿæˆå·¥ä½œæ€»ç»“ï¼ˆOpenAI å…¼å®¹çš„ Chat Completions æ ¼å¼ï¼‰ã€‚
    """
    final_key = deepseek_api_key or os.getenv("DEEPSEEK_API_KEY")
    if not final_key:
        return "é”™è¯¯ï¼šæœªæä¾› DeepSeek API keyã€‚è¯·è®¾ç½®ç¯å¢ƒå˜é‡ DEEPSEEK_API_KEY æˆ–ä½¿ç”¨ --deepseek-key å‚æ•°"

    # æ„å»ºä¸Šä¸‹æ–‡ï¼ˆæ”¯æŒå¤šé¡¹ç›®ï¼‰
    if isinstance(grouped, dict) and grouped and all(isinstance(v, dict) for v in grouped.values()):
        commit_context = build_commit_context_by_project(grouped, details, gap_minutes)  # type: ignore
    else:
        context_lines = []
        for day, items in grouped.items():
            context_lines.append(f"\n## {day} ({len(items)} commits)")
            for c in items:
                sha = c['sha']
                files, ins, dels, body = details.get(sha, ([], 0, 0, ""))
                short_sha = sha[:8]
                time_part = ' '.join(c['date'].split(' ')[1:3]) if ' ' in c['date'] else c['date']
                context_lines.append(f"\n- [{short_sha}] {time_part}")
                context_lines.append(f"  æäº¤ä¿¡æ¯: {c['message']}")
                context_lines.append(f"  ç»Ÿè®¡: {ins} è¡Œæ–°å¢, {dels} è¡Œåˆ é™¤, {len(files)} ä¸ªæ–‡ä»¶")
                if body and body.strip() != c['message']:
                    context_lines.append(f"  è¯¦ç»†å†…å®¹:\n{body}")
                if files:
                    context_lines.append(f"  ä¿®æ”¹çš„æ–‡ä»¶: {', '.join(files[:20])}{' ...' if len(files) > 20 else ''}")
        commit_context = "\n".join(context_lines)
        
    system_msg = system_prompt or default_system_prompt + "\næ­¤å¤–ï¼Œè¯·æŒ‰é¡¹ç›®åˆ†åˆ«ä¼°ç®—æŠ•å…¥æ—¶é—´ï¼ˆæ ¹æ®æäº¤æ—¶é—´å¯†åº¦ä¸è¿ç»­æ€§ï¼‰ï¼Œå¹¶ç»™å‡ºæ¯ä¸ªé¡¹ç›®çš„ä¸»è¦äº§å‡ºã€‚"
    if author:
        system_msg += f"\næ­¤å¤–ï¼Œè¯·åŸºäºä½œè€…å§“åæˆ–é‚®ç®±åŒ…å«â€œ{author}â€çš„æäº¤è¿›è¡Œå·¥ä½œæ€»ç»“ï¼Œå¹¶åœ¨æ‘˜è¦å¼€å¤´æ˜¾å¼æ ‡æ³¨ï¼šä½œè€…ï¼š{author}ã€‚"
        user_msg = f"è¯·æ ¹æ®ä»¥ä¸‹ commit è®°å½•ç”Ÿæˆ{author}å·¥ä½œæ€»ç»“ï¼š\n\n{commit_context}"
        user_msg += """\n\næœ€åè®¡ç®—ä¸€ä¸‹æ•ˆç‡æŒ‡æ•°ï¼ˆPEIï¼‰ï¼š
        è®¾ï¼š
* $N_c$ = å½“æ—¥æäº¤æ¬¡æ•°
* $L_{add}$ = æ–°å¢ä»£ç è¡Œæ•°
* $L_{del}$ = åˆ é™¤ä»£ç è¡Œæ•°
* $T$ = å®é™…æŠ•å…¥æ—¶é—´ï¼ˆå°æ—¶ï¼Œæ’é™¤å¹¶è¡Œé‡å ï¼‰
* $P_{mod}$ = ä¿®æ”¹æ–‡ä»¶æ•°
* $C_{eff}$ = ç¼–è¯‘é€šè¿‡ç‡ï¼ˆæˆ–æµ‹è¯•é€šè¿‡ç‡ï¼Œ0~1ï¼‰
* $C_{cmp}$ = ä»£ç å¤æ‚åº¦ç³»æ•°ï¼ˆ0.5~1.5ï¼Œå¯ä¾æ®ä»»åŠ¡ç±»å‹è°ƒæ•´ï¼‰
---
å…¬å¼ï¼š
$$
\\text{PEI} = \\frac{(0.4 N_c + 0.3 \\log_{10}(L_{add}+L_{del}) + 0.2 \\log_{10}(P_{mod}+1)) \\times C_{eff} \\times C_{cmp}}{T/8}
$$
> è¯´æ˜ï¼š
>
> * å¯¹æ•°é¡¹ä½¿å¾—ä»£ç é‡å’Œæ–‡ä»¶æ•°å¸¦æ¥é€’å‡æ•ˆç›Šï¼Œé˜²æ­¢è¡Œæ•°å †ç§¯é€ æˆè™šé«˜ã€‚
> * $T/8$ ç”¨äºæ—¶é—´å½’ä¸€åŒ–ï¼ˆä»¥ 8 å°æ—¶ä¸ºæ ‡å‡†å·¥ä½œæ—¥ï¼‰ã€‚
> * ç³»æ•°å¯è°ƒï¼š`0.4/0.3/0.2` æƒé‡é€‚åˆä¸­å‹é¡¹ç›®ï¼ˆå¦‚C++å·¥ç¨‹ï¼‰ã€‚
å‚è€ƒè§£é‡Šè¡¨

| PEI å€¼ | æ•ˆç‡ç­‰çº§  | ç‰¹å¾æè¿°           |
| ----- | ----- | -------------- |
| 0â€“3   | ğŸ’¤ ä½æ•ˆ | é¢‘ç¹ä¸Šä¸‹æ–‡åˆ‡æ¢ã€éæ ¸å¿ƒä»»åŠ¡  |
| 4â€“6   | âš™ï¸ æ­£å¸¸ | æŒç»­æ¨è¿›ã€ç¨³å®šäº§å‡º      |
| 7â€“9   | ğŸš€ é«˜æ•ˆ | æ¨¡å—é‡æ„ã€ç³»ç»Ÿä¼˜åŒ–æˆ–å…³é”®ä¿®å¤ |
| â‰¥10   | ğŸ§  å“è¶Š | è‡ªåŠ¨åŒ–ã€ç”Ÿæˆå¼ä»»åŠ¡ã€é›†ä¸­æ”»åš |
        """
    else:
        user_msg = f"è¯·æ ¹æ®ä»¥ä¸‹ commit è®°å½•ç”Ÿæˆå·¥ä½œæ€»ç»“ï¼š\n\n{commit_context}"
    

    # æ˜ å°„æ¨¡å‹åç§°ï¼ˆDeepSeek çš„æ­£ç¡®æ¨¡å‹åç§°ï¼‰
    model_map = {
        "deepseek-chat": "deepseek-chat",
        "deepseek-reasoner": "deepseek-reasoner",
        "chat": "deepseek-chat"  # é»˜è®¤
    }
    actual_model = model_map.get(model.lower(), "deepseek-chat")

    try:
        url = "https://api.deepseek.com/v1/chat/completions"
        headers = {
            "Authorization": f"Bearer {final_key}",
            "Content-Type": "application/json"
        }
        payload = {
            "model": actual_model,
            "messages": [
                {"role": "system", "content": system_msg},
                {"role": "user", "content": user_msg}
            ],
            "temperature": 0.3
        }
        resp = requests.post(url, headers=headers, json=payload, timeout=60)
        resp.raise_for_status()
        data = resp.json()
        return data["choices"][0]["message"]["content"].strip()
    except requests.exceptions.HTTPError as e:
        error_detail = ""
        try:
            error_detail = f" - {resp.text}"
        except:
            pass
        return f"é”™è¯¯ï¼šè°ƒç”¨ DeepSeek API å¤±è´¥: {str(e)}{error_detail}"
    except Exception as e:
        return f"é”™è¯¯ï¼šè°ƒç”¨ DeepSeek API å¤±è´¥: {str(e)}"

def render_markdown_worklog(
    title: str, 
    grouped: Dict[str, List[Dict]], 
    details: Dict[str, Tuple[List[str], int, int, str]],
    add_summary: bool = False,
    summary_text: Optional[str] = None
) -> str:
    lines: List[str] = []
    lines.append(f"# {title}")
    lines.append("")
    total_commits = sum(len(v) for v in grouped.values())
    lines.append(f"æ€»è®¡ {total_commits} ä¸ªæäº¤")
    lines.append("")
    for day, items in grouped.items():
        lines.append(f"## {day} ({len(items)} commits)")
        lines.append("")
        for c in items:
            sha = c['sha']
            short_sha = sha[:8]
            files, ins, dels, body = details.get(sha, ([], 0, 0, ""))
            time_part = ' '.join(c['date'].split(' ')[1:3]) if ' ' in c['date'] else c['date']
            lines.append(f"- [{short_sha}] {time_part} | {c['message']} ({ins}+/{dels}-; {len(files)} files)")
            if files:
                lines.append(f"  - files: {', '.join(files[:10])}{' ...' if len(files) > 10 else ''}")
            if body:
                lines.append("  - message:")
                lines.append("```")
                lines.extend(body.splitlines())
                lines.append("```")
        lines.append("")
    
    # æ·»åŠ æ€»ç»“
    if add_summary and summary_text:
        lines.append(summary_text)
    
    return "\n".join(lines)

def render_multi_project_worklog(title: str, repo_to_grouped: Dict[str, Dict[str, List[Dict]]], repo_to_details: Dict[str, Dict[str, Tuple[List[str], int, int, str]]], add_summary: bool = False, summary_text: Optional[str] = None, gap_minutes: int = 60) -> str:
    lines: List[str] = []
    lines.append(f"# {title}")
    lines.append("")
    total_commits = sum(sum(len(v) for v in grouped.values()) for grouped in repo_to_grouped.values())
    lines.append(f"æ€»è®¡ {total_commits} ä¸ªæäº¤ï¼Œé¡¹ç›®æ•° {len(repo_to_grouped)}")
    lines.append("")
    
    # è®¡ç®—å¹¶è¡Œå·¥ä½œæ—¶é—´
    repo_to_sessions: Dict[str, List[Dict]] = {}
    for repo_name, grouped in repo_to_grouped.items():
        flat_commits: List[Dict] = []
        for items in grouped.values():
            flat_commits.extend(items)
        sessions = compute_work_sessions(flat_commits, gap_minutes)
        repo_to_sessions[repo_name] = sessions
    
    parallel_periods = detect_parallel_sessions(repo_to_sessions)
    if parallel_periods:
        lines.append("## è·¨é¡¹ç›®å¹¶è¡Œå·¥ä½œæ—¶é—´ç»Ÿè®¡")
        total_parallel_minutes = sum(p['duration_minutes'] for p in parallel_periods)
        lines.append(f"æ£€æµ‹åˆ° **{len(parallel_periods)} ä¸ªå¹¶è¡Œå·¥ä½œæ—¶æ®µ**ï¼Œæ€»é‡å æ—¶é•¿çº¦ **{total_parallel_minutes} åˆ†é’Ÿ**")
        lines.append("")
        for idx, p in enumerate(parallel_periods, 1):
            repos_str = ', '.join(p['repos'])
            lines.append(f"- **å¹¶è¡Œæ—¶æ®µ {idx}**ï¼š{p['start'].strftime('%Y-%m-%d %H:%M')} ~ {p['end'].strftime('%Y-%m-%d %H:%M')} ({p['duration_minutes']} åˆ†é’Ÿ)")
            lines.append(f"  - æ¶‰åŠé¡¹ç›®ï¼š{repos_str}")
        lines.append("")
        lines.append("> æ³¨æ„ï¼šå¹¶è¡Œå·¥ä½œæ—¶é—´ä¸åº”ç®€å•ç´¯åŠ ï¼Œå®é™…æŠ•å…¥æ—¶é—´ä»¥é‡å æ—¶æ®µçš„æœ€å¤§å€¼ä¸ºå‡†ã€‚")
        lines.append("")
    
    # å„é¡¹ç›®æ—¶é—´ç»Ÿè®¡
    lines.append("## å„é¡¹ç›®æ—¶é—´ç»Ÿè®¡")
    for repo_name, grouped in repo_to_grouped.items():
        sessions = repo_to_sessions[repo_name]
        if sessions:
            total_minutes = sum(s['duration_minutes'] for s in sessions)
            lines.append(f"### {repo_name}")
            lines.append(f"- å·¥ä½œä¼šè¯ï¼š{len(sessions)} ä¸ªï¼Œæ€»æ—¶é•¿çº¦ {total_minutes} åˆ†é’Ÿ")
            for idx, s in enumerate(sessions, 1):
                is_parallel = any(
                    not (s['end'] < pp['start'] or s['start'] > pp['end'])
                    for pp in parallel_periods
                    if repo_name in pp['repos']
                )
                parallel_marker = " **[å¹¶è¡Œ]**" if is_parallel else ""
                lines.append(f"  - ä¼šè¯{idx}ï¼š{s['start'].strftime('%H:%M')} ~ {s['end'].strftime('%H:%M')} ({s['duration_minutes']} åˆ†é’Ÿ, {len(s['commits'])} æ¬¡æäº¤){parallel_marker}")
    lines.append("")
    for repo_name, grouped in repo_to_grouped.items():
        lines.append(f"# é¡¹ç›®ï¼š{repo_name}")
        lines.append("")
        for day, items in grouped.items():
            lines.append(f"## {day} ({len(items)} commits)")
            lines.append("")
            for c in items:
                sha = c['sha']
                short_sha = sha[:8]
                files, ins, dels, body = repo_to_details[repo_name].get(sha, ([], 0, 0, ""))
                time_part = ' '.join(c['date'].split(' ')[1:3]) if ' ' in c['date'] else c['date']
                lines.append(f"- [{short_sha}] {time_part} | {c['message']} ({ins}+/{dels}-; {len(files)} files)")
                if files:
                    lines.append(f"  - files: {', '.join(files[:10])}{' ...' if len(files) > 10 else ''}")
                if body:
                    lines.append("  - message:")
                    lines.append("```")
                    lines.extend(body.splitlines())
                    lines.append("```")
            lines.append("")
        lines.append("")
    if add_summary and summary_text:
        lines.append(summary_text)
    return "\n".join(lines)

def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Generate work log from git commits")
    parser.add_argument('--repo', type=str, default=None, help='Path to git repository (single)')
    parser.add_argument('--repos', type=str, default=None, help='Multiple repositories, comma-separated')
    parser.add_argument('--since', type=str, default=None, help='Start datetime (ISO or YYYY-MM-DD)')
    parser.add_argument('--until', type=str, default=None, help='End datetime (ISO or YYYY-MM-DD)')
    parser.add_argument('--days', type=int, default=None, help='If set, use last N days ending today')
    parser.add_argument('--session-gap-minutes', type=int, default=60, help='Gap minutes to split work sessions')
    parser.add_argument('--author', type=str, default=None, help='Filter by author name or email (optional)')
    parser.add_argument('--output', type=str, default=None, help='Output file path (.md). If not set, print to stdout')
    parser.add_argument('--title', type=str, default=None, help='Title for the work log document')
    parser.add_argument('--add-summary', action='store_true', help='Add AI-generated Chinese summary at the end')
    parser.add_argument('--openai-key', type=str, default=None, help='OpenAI API key (or set OPENAI_API_KEY env var)')
    parser.add_argument('--openai-model', type=str, default='gpt-4o-mini', help='OpenAI model to use (default: gpt-4o-mini)')
    parser.add_argument('--system-prompt-file', type=str, default=None, help='Path to custom system prompt file')
    # DeepSeek æ”¯æŒ
    parser.add_argument('--provider', type=str, default='openai', choices=['openai', 'deepseek'], help='LLM provider')
    parser.add_argument('--deepseek-key', type=str, default=None, help='DeepSeek API key (or set DEEPSEEK_API_KEY env var)')
    parser.add_argument('--deepseek-model', type=str, default='deepseek-chat', help='DeepSeek model (e.g., deepseek-chat, deepseek-reasoner)')
    return parser.parse_args()

def parse_date_input(value: Optional[str], default_dt: Optional[datetime]) -> Optional[datetime]:
    if value is None:
        return default_dt
    # try ISO then YYYY-MM-DD
    try:
        return datetime.fromisoformat(value)
    except Exception:
        try:
            return datetime.strptime(value, "%Y-%m-%d")
        except Exception:
            raise ValueError(f"æ— æ³•è§£ææ—¥æœŸ: {value}")

def git2work():
    args = parse_args()
    repo_paths: List[str] = []
    if args.repos:
        repo_paths = [p.strip() for p in args.repos.split(',') if p.strip()]
    elif args.repo:
        repo_paths = [args.repo]
    else:
        # fallback to default single repo if none provided
        repo_paths = ["/mnt/d/works/RayTracy"]

    now = datetime.now()
    if args.days is not None and args.days > 0:
        start = (now - timedelta(days=args.days - 1)).replace(hour=0, minute=0, second=0, microsecond=0)
        end = now.replace(hour=23, minute=59, second=59, microsecond=0)
    else:
        start = parse_date_input(args.since, now.replace(hour=0, minute=0, second=0, microsecond=0))
        end = parse_date_input(args.until, now.replace(hour=23, minute=59, second=59, microsecond=0))
        if start is not None:
            start = start.replace(hour=0, minute=0, second=0, microsecond=0)
        if end is not None:
            end = end.replace(hour=23, minute=59, second=59, microsecond=0)

    multi_project = len(repo_paths) > 1

    if not multi_project:
        repo = repo_paths[0]
        commits = get_commits_between(repo, start, end)
        if args.author:
            author_lower = args.author.lower()
            commits = [c for c in commits if author_lower in c['author_name'].lower() or author_lower in c['author_email'].lower()]
        details: Dict[str, Tuple[List[str], int, int, str]] = {}
        for c in commits:
            files, ins, dels = get_commit_numstat(repo, c['sha'])
            body = get_commit_body(repo, c['sha'])
            details[c['sha']] = (files, ins, dels, body)
        grouped = group_commits_by_date(commits)
    else:
        repo_to_commits: Dict[str, List[Dict]] = {}
        repo_to_details: Dict[str, Dict[str, Tuple[List[str], int, int, str]]] = {}
        repo_to_grouped: Dict[str, Dict[str, List[Dict]]] = {}
        for repo in repo_paths:
            commits = get_commits_between(repo, start, end)
            if args.author:
                author_lower = args.author.lower()
                commits = [c for c in commits if author_lower in c['author_name'].lower() or author_lower in c['author_email'].lower()]
            repo_to_commits[repo] = commits
            details_map: Dict[str, Tuple[List[str], int, int, str]] = {}
            for c in commits:
                files, ins, dels = get_commit_numstat(repo, c['sha'])
                body = get_commit_body(repo, c['sha'])
                details_map[c['sha']] = (files, ins, dels, body)
            repo_to_details[repo] = details_map
            repo_to_grouped[repo] = group_commits_by_date(commits)
        grouped = repo_to_grouped  # type: ignore
        details = repo_to_details  # type: ignore

    title = args.title or (f"Work Log: {start.date()} to {end.date()}" if start and end else "Work Log")
    
    # ç”Ÿæˆæ€»ç»“ï¼ˆå¦‚æœéœ€è¦ï¼‰
    summary_text = None
    if args.add_summary:
        print("æ­£åœ¨ç”Ÿæˆ AI æ€»ç»“...")
        # è¯»å–è‡ªå®šä¹‰æç¤ºè¯ï¼ˆå¦‚æœæœ‰ï¼‰
        system_prompt = None
        if args.system_prompt_file and os.path.exists(args.system_prompt_file):
            with open(args.system_prompt_file, 'r', encoding='utf-8') as f:
                system_prompt = f.read()
        
        if getattr(args, 'provider', 'openai') == 'deepseek':
            summary_text = generate_summary_with_deepseek(
                grouped,  # type: ignore
                details,  # type: ignore
                system_prompt=system_prompt,
                deepseek_api_key=args.deepseek_key,
                model=args.deepseek_model,
                author=args.author,
                gap_minutes=args.session_gap_minutes
            )
        else:
            summary_text = generate_summary_with_openai(
                grouped,  # type: ignore
                details,  # type: ignore
                system_prompt=system_prompt,
                openai_api_key=args.openai_key,
                model=args.openai_model,
                author=args.author,
                gap_minutes=args.session_gap_minutes
            )
        print("AI æ€»ç»“ç”Ÿæˆå®Œæˆ")
    
    if not multi_project:
        md = render_markdown_worklog(title, grouped, details, add_summary=args.add_summary, summary_text=summary_text)  # type: ignore
    else:
        md = render_multi_project_worklog(title, grouped, details, add_summary=args.add_summary, summary_text=summary_text, gap_minutes=args.session_gap_minutes)  # type: ignore

    if args.output:
        os.makedirs(os.path.dirname(args.output), exist_ok=True) if os.path.dirname(args.output) else None
        with open(args.output, 'w', encoding='utf-8') as f:
            f.write(md)
        print(f"å·²å†™å…¥: {args.output}")
    else:
        print(md)

if __name__ == "__main__":
    git2work()